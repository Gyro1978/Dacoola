{
    "id": "7b9c794995ed7a4ece7a1a4c41998c49c23ee51b9156f1c4974bf5d18a3f8437",
    "title": "Project Petridish: Efficient forward neural architecture search",
    "link": "https://www.microsoft.com/en-us/research/blog/project-petridish-efficient-forward-neural-architecture-search/",
    "published_iso": "2019-12-09T16:16:44Z",
    "summary": "<p><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/MSR_NeurIPS_EfficientForward_1400x788.gif\"><img alt=\"Animation depicting Efficient forward neural architecture search\" class=\"aligncenter size-full wp-image-625539\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/MSR_NeurIPS_EfficientForward_1400x788.gif\" width=\"1400\" /><span class=\"sr-only\"> (opens in new tab)</span></a></p>\n<p>Having experience in deep learning doesn’t hurt when it comes to the often mysterious, time- and cost-consuming process of hunting down an appropriate neural architecture. But truth be told, no one really knows what works the best on a new dataset and task. Relying on well-known, top-performing networks provides few guarantees in a space where your dataset can look very different from anything those proven networks have encountered before. For example, a network that worked well on satellite images won’t necessarily work well on the selfies and food photos making the rounds on social media. Even when a task dataset is similar to other common datasets and a bit of prior knowledge can be utilized by starting with similar architectures, it’s challenging to find architectures that satisfy not only accuracy, but also memory and latency constraints, among others, at serving time. These challenges could lead to a frustrating amount of trial and error.</p>\n<p>In our paper <a href=\"https://www.microsoft.com/en-us/research/publication/efficient-forward-architecture-search/\">“Efficient Forward Architecture Search,”<span class=\"sr-only\"> (opens in new tab)</span></a> which is being presented at the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://neurips.cc/\">33rd Conference on Neural Information Processing Systems (NeurIPS)<span class=\"sr-only\"> (opens in new tab)</span></a>, we introduce Petridish, a neural architecture search algorithm that opportunistically adds new layers determined to be beneficial to a parent model, resulting in a <em>gallery</em> of models capable of satisfying a variety of constraints for researchers and engineers to choose from. The team behind the ongoing work is comprised of myself, Carnegie Mellon University PhD graduate <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.linkedin.com/in/hanzhang-hu-76487562/\">Hanzhang Hu<span class=\"sr-only\"> (opens in new tab)</span></a>, and <a href=\"https://www.microsoft.com/en-us/research/people/jcl/\">John Langford<span class=\"sr-only\"> (opens in new tab)</span></a>, Partner Research Manager; <a href=\"https://www.microsoft.com/en-us/research/people/rcaruana/\">Rich Caruana<span class=\"sr-only\"> (opens in new tab)</span></a>, Senior Principal Researcher; <a href=\"https://www.microsoft.com/en-us/research/people/shitals/\">Shital Shah<span class=\"sr-only\"> (opens in new tab)</span></a>, Principal Research Software Engineer; <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.linkedin.com/in/saurajitm/\">Saurajit Mukherjee<span class=\"sr-only\"> (opens in new tab)</span></a>, Principal Engineering Manager; and <a href=\"https://www.microsoft.com/en-us/research/people/horvitz/\">Eric Horvitz<span class=\"sr-only\"> (opens in new tab)</span></a>, Technical Fellow and Director, Microsoft Research AI.</p>\n<p>With Petridish, we seek to increase efficiency and speed in finding suitable neural architectures, making the process easier for those in the field, as well as those without expertise interested in machine learning solutions.</p>\n<h3><strong>Neural architecture search—forward search vs. backward search</strong></h3>\n<p>The machine learning subfield of neural architecture search (NAS) aims to take the guesswork out of people’s hands and let algorithms search for good architectures. While <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/1611.01578\">NAS experienced a resurgence in 2016<span class=\"sr-only\"> (opens in new tab)</span></a> and has become a very popular topic (see the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.automl.org/automl/literature-on-neural-architecture-search/\">AutoML Freiburg-Hannover website<span class=\"sr-only\"> (opens in new tab)</span></a> for a continuously updated compilation of published papers), the earliest papers on the topic date back to <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://papers.nips.cc/paper/149-self-organizing-neural-networks-for-the-identification-problem\">NeurIPS 1988<span class=\"sr-only\"> (opens in new tab)</span></a> and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture\">NeurIPS 1989<span class=\"sr-only\"> (opens in new tab)</span></a>. Most of the well-known NAS algorithms today, such as <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/1802.03268\">Efficient Neural Architecture Search (ENAS)<span class=\"sr-only\"> (opens in new tab)</span></a>, <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/1806.09055\">Differentiable Architecture Search (DARTS)<span class=\"sr-only\"> (opens in new tab)</span></a>, and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/1812.00332\">ProxylessNAS<span class=\"sr-only\"> (opens in new tab)</span></a>, are examples of backward search. During backward search, smaller networks are sampled from a supergraph, a large architecture containing multiple subarchitectures. A limitation of backward search algorithms is that human domain knowledge is needed to create a supergraph in the first place. In contrast, Petridish is an example of forward search, a paradigm first introduced 30 years ago by Scott Fahlman and Christian Lebiere of Carnegie Mellon University in that 1989 NAS NeurIPS paper. Forward search requires far less human knowledge when it comes to search space design.</p>\n<p>Petridish, which was also inspired by gradient boosting, creates as its search output a gallery of models to choose from, incorporates stop-forward and stop-gradient layers in more efficiently identifying beneficial candidates for building that gallery, and uses asynchronous training.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_625275\" style=\"width: 550px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/Workflow.png\"><img alt=\"Figure 1: Petridish, a neural architecture search algorithm that grows a nominal seed model during search by opportunistically adding layers as needed, comprises three phases.\" class=\" wp-image-625275\" height=\"281\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/Workflow.png\" width=\"540\" /><p class=\"wp-caption-text\" id=\"caption-attachment-625275\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 1: Petridish, a neural architecture search algorithm that grows a nominal seed model during search by opportunistically adding layers as needed, comprises three phases. Phase 0 starts with the small parent model. In Phase 1, a large number of candidates is considered for addition to the parent. If a candidate is promising, then it’s added to the parent in Phase 2. Models in Phase 2 that lie near the boundary of the current estimate of the Pareto frontier (see Figure 2) are then added to the pool of parent models in Phase 0 so they have the chance to grow further.</p></div></p>\n<h3><strong>Overview of Petridish </strong></h3>\n<p>There are three main phases to Petridish:</p>\n<ul>\n<li>\n<blockquote><p><strong>PHASE 0</strong>: We start with some parent model, a very small human-written model with one or two layers or a model already found by domain experts on a dataset.</p></blockquote>\n</li>\n<li>\n<blockquote><p><strong>PHASE 1</strong>: We connect the candidate layers to the parent model using <em>stop-gradient</em> and <em>stop-forward</em> layers and partially train it. The candidate layers can be any bag of operations in the search space. For example, for vision tasks, we set the candidates to be 3×3 and 5×5 dilated convolutions, 3×3 and 5×5 separable convolutions, 3×3 max pooling, 3×3 average pooling, and identity. Using stop-gradient and stop-forward layers allows gradients with respect to the candidates to be accumulated without affecting the model’s forward activations and backward gradients. Without the stop-gradient and stop-forward layers, it would be difficult to determine which candidate layers are contributing what to the parent model’s performance and would require separate training if you wanted to see their respective contributions, increasing costs. By leaving the parent model unaffected by the candidate layers, we’re able to independently evaluate each candidate simultaneously.</p></blockquote>\n</li>\n<li>\n<blockquote><p><strong>PHASE 2:</strong> If a particular candidate or set of candidates is found to be beneficial to the model, then we remove the stop-gradient and stop-forward layers and the other candidates and train the model to convergence. The training results are added to a scatterplot, naturally creating an estimate of the Pareto frontier. A Pareto frontier encodes the relationship between different objectives of a multi-objective optimization problem where there can’t be gains in one objective without giving up something in the other. Only those models that have a realistic chance of improving the estimate of the Pareto frontier get moved to the parent queue in Phase 0.</p></blockquote>\n</li>\n</ul>\n<p>Explicitly maintaining a Pareto frontier, like the one represented by Figure 2, allows researchers, engineers, and product groups to more easily determine the architecture that achieves the best combination of properties they’re considering for a particular task. With Figure 2, for example, they can more easily answer questions such as what is the best-performing architecture available given a certain amount of floating-point operations per second (FLOPS) at serving time. This is crucial in production environments, where accuracy, FLOPS, and other metrics like serving latency, memory, and cost are important considerations. Once a search has been completed, if the need for a model meeting different constraints arises, all the team has to do is look it up on the plot without having to redo the architecture hunt.</p>\n<p>All three phases are executing concurrently in a distributed manner with each phase maintaining its own queue of models where each queue is cleared by a pool of worker processes in parallel.</p>\n<p> </p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_626235\" style=\"width: 915px;\"><img alt=\"\" class=\"wp-image-626235 size-full\" height=\"700\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/Pareto.png\" width=\"905\" /><p class=\"wp-caption-text\" id=\"caption-attachment-626235\">Figure 2: Petridish maintains an estimate of the Pareto frontier and makes it easier to see the tradeoff between accuracy, FLOPS, memory, latency, and other criteria. Those models along the Pareto frontier (red line) make up the search output, a gallery of models from which researchers and engineers can choose.</p></div></p>\n<h3><strong>Candidate selection</strong></h3>\n<p>In Phase 1, for a candidate to be selected for incorporation into the model, we apply L1 regularization to all the candidates and greedily select the candidates that have the highest weight. L1 regularization is commonly used in feature selection to induce sparsity over a set of features so that one can effectively get the most predictive power out of the least number of additional features. Petridish should remind some readers of gradient boosted machines (GBMs), where additional capacity is sequentially added—for example, in a gradient boosted forest—to minimize residual loss.</p>\n<p>The construction of Petridish makes it particularly amenable to warm-starting from a previously known model, which is important, as datasets continually change in size and character, a common occurrence in production environments.</p>\n<h3><strong>Summary of results </strong></h3>\n<p>On <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10<span class=\"sr-only\"> (opens in new tab)</span></a>, Petridish achieves 2.75 ±0.21 percent average test error, with 2.51 percent as the best result, using only 3.2M parameters and five GPU days of search time on the popular cell search space. On the more general and bigger macro search space, Petridish achieves 2.85 ±0.12 percent average test error, with 2.83 percent as the best search, using only 2.2M parameters. This is state of the art on a much bigger search space at a similar number of parameters and dispels the common myth that macro search spaces are difficult to deal with and cannot easily achieve competitive performance, opening the door to interesting families of architectures researchers might not have previously considered.</p>\n<p>On transferring the models found on CIFAR-10 to <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://www.image-net.org/\">ImageNet<span class=\"sr-only\"> (opens in new tab)</span></a>, Petridish achieves 28.7 ±0.15 percent top-1 test error, with 28.5 percent as the best result, using only 4.3M parameters on the macro search space. On the cell search space, Petridish achieves 26.3 ±0.20 percent top-1 test error, with 26.0 percent as the best result, using 4.8M parameters. Again, we show that macro search spaces that don’t need a prior human-designed supergraph can be quite competitive, and more research to unlock performance from such expressive spaces is needed.</p>\n<p>While we’ve demonstrated Petridish on CIFAR-10/100, ImageNet and also <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html\">Penn Treebank<span class=\"sr-only\"> (opens in new tab)</span></a>, which are commonly accepted NAS datasets, we’re trying it out on a number of diverse datasets in vision and language and invite the community to do the same and report back their experiences. All <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/petridishnn\">source code for Petridish is openly available<span class=\"sr-only\"> (opens in new tab)</span></a> (under MIT license) using TensorFlow 1.12. We’re writing a more robust distributed version in PyTorch, which will appear shortly at the same repository.</p>\n<p><em>This work was spearheaded by Hanzhang Hu, a Carnegie Mellon University PhD graduate, during a Microsoft Research summer internship. Team members Debadeepta Dey, John Langford, Rich Caruana, and Eric Horvitz served as advisors on the work.</em><span class=\"sr-only\" id=\"label-external-link\">Opens in a new tab</span></p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/project-petridish-efficient-forward-neural-architecture-search/\">Project Petridish: Efficient forward neural architecture search</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
    "source_feed": "https://www.microsoft.com/en-us/research/blog/category/artificial-intelligence/feed/",
    "scraped_at_iso": "2025-05-03T23:50:16Z",
    "selected_image_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/MSR_NeurIPS_EfficientForward_1400x788.png",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "Research",
        "reasoning_summary": "The article discusses a novel approach to neural architecture search, which is a significant research topic in AI, presented by Microsoft with verifiable content.",
        "primary_topic_keyword": "neural architecture search"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-03T23:52:17Z",
    "topic": "Research",
    "is_breaking": false,
    "primary_keyword": "neural architecture search",
    "seo_agent_results": {
        "generated_title_tag": "Microsoft's Petridish Advances Neural Architecture Search",
        "generated_meta_description": "Microsoft introduces Petridish, an efficient neural architecture search method, reducing trial and error in AI model development.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"Microsoft's Petridish Advances Neural Architecture Search\",  \n  \"description\": \"Microsoft introduces Petridish, an efficient neural architecture search method, reducing trial and error in AI model development.\",  \n  \"keywords\": [\"neural architecture search\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://www.microsoft.com/en-us/research/blog/project-petridish-efficient-forward-neural-architecture-search/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/12/MSR_NeurIPS_EfficientForward_1400x788.png\" },  \n  \"datePublished\": \"2019-12-09T16:16:44Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://dacoolaa.netlify.app\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Project Petridish: Efficient Forward Neural Architecture Search  \n\nMicrosoft Research has unveiled **Petridish**, a novel **neural architecture search (NAS)** algorithm designed to streamline the process of finding optimal AI models. Unlike traditional backward-search methods, Petridish employs forward search, dynamically adding beneficial layers to a parent model while minimizing human intervention. The approach generates a gallery of models optimized for accuracy, memory, and latency, addressing challenges in adapting architectures to diverse datasets.  \n\n### Why It Matters  \n\nNeural architecture search is critical for AI efficiency, but existing methods often require extensive manual tuning. Petridish’s forward-search paradigm reduces dependency on pre-defined supergraphs, making it more adaptable for real-world applications. By maintaining a Pareto frontier of model trade-offs, it empowers researchers to select architectures tailored to specific constraints—accelerating AI deployment in production environments."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "Neural Architecture Search",
        "Microsoft Research",
        "Petridish Algorithm",
        "AI Model Optimization",
        "Forward Search",
        "Machine Learning Efficiency",
        "AI Deployment",
        "Pareto Frontier",
        "Deep Learning"
    ],
    "tags_agent_error": null,
    "trend_score": 9.5,
    "audio_url": null
}