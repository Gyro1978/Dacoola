{
    "id": "860921c3caffde2284c7c714c779e46932f59617efee90984ff800e62d1068bd",
    "title": "Chill Factor: NVIDIA Blackwell Platform Boosts Water Efficiency by Over 300x",
    "link": "https://blogs.nvidia.com/blog/blackwell-platform-water-efficiency-liquid-cooling-data-centers-ai-factories/",
    "published_iso": "2025-04-22T13:00:39Z",
    "summary": "<div id=\"bsf_rt_marker\"></div><p>Traditionally, data centers have relied on air cooling — where mechanical chillers circulate chilled air to absorb heat from servers, helping them maintain optimal conditions. But as AI models increase in size, and the use of AI reasoning models rises, maintaining those optimal conditions is not only getting harder and more expensive — but more energy-intensive.</p>\n<p>While data centers once operated at 20 kW per rack, today’s hyperscale facilities can support over 135 kW per rack, making it an order of magnitude harder to dissipate the heat generated by high-density racks. To keep AI servers running at peak performance, a new approach is needed for efficiency and scalability.</p>\n<p>One key solution is liquid cooling — by reducing dependence on chillers and enabling more efficient heat rejection, liquid cooling is driving the next generation of high-performance, energy-efficient AI infrastructure.</p>\n<p>The <a href=\"https://www.nvidia.com/en-us/data-center/gb200-nvl72/\" target=\"_blank\">NVIDIA GB200 NVL72</a> and the <a href=\"https://www.nvidia.com/en-us/data-center/gb300-nvl72/\" target=\"_blank\">NVIDIA GB300 NVL72</a> are rack-scale, liquid-cooled systems designed to handle the demanding tasks of trillion-parameter large language model inference. Their architecture is also specifically optimized for <a href=\"https://blogs.nvidia.com/blog/ai-scaling-laws/#:~:text=The%20recent%20rise%20of%20test,required%20to%20solve%20a%20task.\">test-time scaling</a> accuracy and performance, making it an ideal choice for running AI reasoning models while efficiently managing energy costs and heat.</p>\n<figure class=\"wp-caption alignnone\" id=\"attachment_80021\" style=\"width: 1680px;\"><img alt=\"\" class=\"wp-image-80021 size-large\" height=\"1261\" src=\"https://blogs.nvidia.com/wp-content/uploads/2025/04/liquid-cooled-nvidia-blackwell-compute-tray-1680x1261.jpg\" width=\"1680\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-80021\">Liquid-cooled NVIDIA Blackwell compute tray.</figcaption></figure>\n<h2><strong>Driving Unprecedented Water Efficiency and Cost Savings in AI Data Centers</strong></h2>\n<p>Historically, cooling alone has accounted for up to <a href=\"https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy\" target=\"_blank\">40% of a data center’s electricity consumption</a>, making it one of the most significant areas where efficiency improvements can drive down both operational expenses and energy demands.</p>\n<p>Liquid cooling helps mitigate costs and energy use by capturing heat directly at the source. Instead of relying on air as an intermediary, direct-to-chip liquid cooling transfers heat in a technology cooling system loop. That heat is then cycled through a coolant distribution unit via liquid-to-liquid heat exchanger, and ultimately transferred to a facility cooling loop. Because of the higher efficiency of this heat transfer, data centers and AI factories can operate effectively with warmer water temperatures — reducing or eliminating the need for mechanical chillers in a wide range of climates.</p>\n<p>The <a href=\"https://www.nvidia.com/en-us/data-center/gb200-nvl72/\" target=\"_blank\">NVIDIA GB200 NVL72</a> rack-scale, liquid-cooled system, built on the NVIDIA Blackwell platform, offers exceptional performance while balancing energy costs and heat. It packs unprecedented compute density into each server rack, delivering 40x higher revenue potential, 30x higher throughput, 25x more <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\" target=\"_blank\">energy efficiency</a> and 300x more water efficiency than traditional air-cooled architectures. Newer NVIDIA GB300 NVL72 systems built on the Blackwell Ultra platform boast a 50x higher revenue potential and 35x higher throughput with 30x more energy efficiency.</p>\n<p>Data centers spend an estimated <a href=\"https://www.cbre.com/press-releases/north-american-data-center-pricing-nears-record-highs-driven-by-demand-limited-availablily\" target=\"_blank\">$1.9-2.8M per megawatt (MW) per year</a>, which amounts to nearly $500,000 spent annually on cooling-related energy and water costs. By deploying the liquid-cooled GB200 NVL72 system, hyperscale data centers and <a href=\"https://blogs.nvidia.com/blog/ai-factory/\">AI factories</a> can achieve up to 25x cost savings, leading to over $4 million dollars in annual savings for a 50 MW hyperscale data center.</p>\n<p>For data center and <a href=\"https://www.nvidia.com/en-us/glossary/ai-factory/\" target=\"_blank\">AI factory</a> operators, this means lower operational costs, <a href=\"https://blogs.nvidia.com/blog/datacenter-efficiency-metrics-isc/\">enhanced energy efficiency metrics</a> and a future-proof infrastructure that scales AI workloads efficiently — without the unsustainable water footprint of legacy cooling methods.</p>\n<h2><strong>Moving Heat Outside the Data Center</strong></h2>\n<p>As compute density rises and AI workloads drive unprecedented thermal loads, data centers and AI factories must rethink how they remove heat from their infrastructure. The traditional methods of heat rejection that supported predictable CPU-based scaling are no longer sufficient on their own. Today, there are multiple options for moving heat outside the facility, but four major categories dominate current and emerging deployments.</p>\n<h3><strong>Key Cooling Methods in a Changing Landscape</strong></h3>\n<ul>\n<li><b>Mechanical Chillers</b>: Mechanical chillers use a vapor compression cycle to cool water, which is then circulated through the data center to absorb heat. These systems are typically air-cooled or water-cooled, with the latter often paired with cooling towers to reject heat. While chillers are reliable and effective across diverse climates, they are also highly energy-intensive. In AI-scale facilities where power consumption and sustainability are top priorities, reliance on chillers can significantly impact both operational costs and carbon footprint.</li>\n<li><b>Evaporative Cooling</b>: Evaporative cooling uses the evaporation of water to absorb and remove heat. This can be achieved through direct or indirect systems, or hybrid designs. These systems are much more energy-efficient than chillers but come with high water consumption. In large facilities, they can consume millions of gallons of water per megawatt annually. Their performance is also climate-dependent, making them less effective in humid or water-restricted regions.</li>\n<li><b>Dry Coolers</b>: Dry coolers remove heat by transferring it from a closed liquid loop to the ambient air using large finned coils, much like an automotive radiator. These systems don’t rely on water and are ideal for facilities aiming to reduce water usage or operate in dry climates. However, their effectiveness depends heavily on the temperature of the surrounding air. In warmer environments, they may struggle to keep up with high-density cooling demands unless paired with liquid-cooled IT systems that can tolerate higher operating temperatures.</li>\n<li><b>Pumped Refrigerant Systems: </b>Pumped refrigerant systems use liquid refrigerants to move heat from the data center to outdoor heat exchangers. Unlike chillers, these systems don’t rely on large compressors inside the facility and they operate without the use of water. This method offers a thermodynamically efficient, compact and scalable solution that works especially well for edge deployments and water-constrained environments. Proper refrigerant handling and monitoring are required, but the benefits in power and water savings are significant.</li>\n</ul>\n<p>Each of these methods offers different advantages depending on factors like climate, rack density, facility design and sustainability goals. As liquid cooling becomes more common and servers are designed to operate with warmer water, the door opens to more efficient and environmentally friendly cooling strategies — reducing both energy and water use while enabling higher compute performance.</p>\n<h2><strong>Optimizing Data Centers for AI Infrastructure</strong></h2>\n<p>As AI workloads grow exponentially, operators are reimagining data center design with infrastructure built specifically for high-performance AI and energy efficiency. Whether they’re transforming their entire setup into dedicated AI factories or upgrading modular components, optimizing <a href=\"https://developer.nvidia.com/deep-learning-performance-training-inference/ai-inference\" target=\"_blank\">inference performance</a> is crucial for managing costs and operational efficiency.</p>\n<p>To get the best performance, high compute capacity GPUs aren’t enough — they need to be able to communicate with each other at lightning speed.</p>\n<p><a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\" target=\"_blank\">NVIDIA NVLink</a> boosts communication, enabling GPUs to operate as a massive, tightly integrated processing unit for maximum performance with a full-rack power density of 120 kW. This tight, high-speed communication is crucial for today’s AI tasks, where every second saved on transferring data can mean more <a href=\"https://blogs.nvidia.com/blog/ai-tokens-explained/\">tokens</a> per second and more efficient AI models.</p>\n<p>Traditional air cooling struggles at these power levels. To keep up, data center air would need to be either cooled to below-freezing temperatures or flow at near-gale speeds to carry the heat away, making it increasingly impractical to cool dense racks with air alone.</p>\n<p>At nearly 1,000x the density of air, liquid cooling excels at carrying heat away thanks to its superior heat capacitance and thermal conductivity. By efficiently transferring heat away from high-performance GPUs, liquid cooling reduces reliance on energy-intensive and noisy cooling fans, allowing more power to be allocated to computation rather than cooling overhead.</p>\n<h3><strong>Liquid Cooling in Action</strong></h3>\n<p>Innovators across the industry are leveraging liquid cooling to slash energy costs, improve density and drive AI efficiency:</p>\n<ul>\n<li><a href=\"https://www.vertiv.com/en-us/about/news-and-insights/corporate-news/vertiv-codevelops-with-nvidia-complete-power-and-cooling-blueprint-for--nvidia-gb200-nvl72-platform/\" target=\"_blank\">Vertiv’s reference architecture for NVIDIA GB200 NVL72</a> servers reduces annual energy consumption by 25%, cuts rack space requirements by 75% and shrinks the power footprint by 30%.</li>\n<li><a href=\"https://www.se.com/ww/en/about-us/newsroom/news/press-releases/schneider-electric-announces-new-solutions-to-address-the-energy-and-sustainability-challenges-spurred-by-ai-674f09f25ad2a1e7eb07b6f3\" target=\"_blank\">Schneider Electric’s liquid-cooling infrastructure supports up to 132 kW per rack</a>, improving energy efficiency, scalability and overall performance for GB200 NVL72 AI data centers.</li>\n<li><a href=\"https://www.coolitsystems.com/coolit-systems-announces-further-breakthroughs-in-row-based-coolant-distribution-unit-performance/\" target=\"_blank\">CoolIT Systems’ high-density CHx2000 liquid-to-liquid coolant distribution units provide 2MW cooling capacity</a> at 5°C approach temperature, ensuring reliable thermal management for GB300 NVL72 deployments. Also, CoolIT Systems’ OMNI All-Metal Coldplates with patented Split-Flow technology provide <a href=\"https://www.coolitsystems.com/4000watt/\" target=\"_blank\">targeted cooling of over 4,000W</a> thermal design power while reducing pressure drop.</li>\n<li><a href=\"https://www.boydcorp.com/blog/cooling-nvidia-gb200-nvl72-artificial-intelligence.html\" target=\"_blank\">Boyd’s advanced liquid-cooling solutions</a>, incorporating the company’s over two decades of high-performance compute industry experience, include coolant distribution units, liquid-cooling loops and cold plates to further maximize energy efficiency and system reliability for high-density AI workloads.</li>\n</ul>\n<p>Cloud service providers are also adopting cutting-edge cooling and power innovations. Next-generation AWS data centers, featuring jointly developed liquid cooling solutions, <a href=\"https://press.aboutamazon.com/2024/12/aws-announces-new-data-center-components-to-support-ai-innovation-and-further-improve-energy-efficiency\" target=\"_blank\">increase compute power by 12% while reducing energy consumption by up to 46%</a> — all while maintaining water efficiency.</p>\n<h2><strong>Cooling the AI Infrastructure of the Future</strong></h2>\n<p>As AI continues to push the limits of computational scale, innovations in cooling will be essential to meeting the thermal management challenges of the post-Moore’s law era.</p>\n<p>NVIDIA is leading this transformation through initiatives like the <a href=\"https://blogs.nvidia.com/blog/liquid-cooling-doe-challenge/\">COOLERCHIPS program</a>, a U.S. Department of Energy-backed effort to develop modular data centers with next-generation cooling systems that are projected to reduce costs by at least 5% and improve efficiency by 20% over traditional air-cooled designs.</p>\n<p>Looking ahead, data centers must evolve not only to support AI’s growing demands but do so sustainably — maximizing energy and water efficiency while minimizing environmental impact. By embracing high-density architectures and advanced liquid cooling, the industry is paving the way for a more efficient AI-powered future.</p>\n<p><em>Learn more about breakthrough <a href=\"https://www.nvidia.com/gtc/sessions/sustainable-computing/\" target=\"_blank\">solutions for data center energy and water efficiency</a> presented at NVIDIA GTC 2025 and discover how accelerated computing is driving a more efficient future with <a href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\" target=\"_blank\">NVIDIA Blackwell</a>.</em></p>",
    "source_feed": "https://blogs.nvidia.com/feed/",
    "scraped_at_iso": "2025-05-03T03:46:23Z",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "Hardware",
        "reasoning_summary": "The article discusses NVIDIA's Blackwell Platform significantly improving water efficiency in data centers, a verifiable and notable advancement in AI infrastructure hardware.",
        "primary_topic_keyword": "NVIDIA Blackwell cooling"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-03T03:47:30Z",
    "topic": "Hardware",
    "is_breaking": false,
    "primary_keyword": "NVIDIA Blackwell cooling",
    "similarity_check_error": null,
    "selected_image_url": "https://blogs.nvidia.com/wp-content/uploads/2025/04/water-efficiency-hero-resized.jpg",
    "seo_agent_results": {
        "generated_title_tag": "NVIDIA Blackwell Cooling Boosts Data Center Water Efficiency 300x",
        "generated_meta_description": "NVIDIA Blackwell cooling solutions enhance water efficiency by 300x in AI data centers, reducing costs and energy consumption with liquid cooling.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"NVIDIA Blackwell Cooling Boosts Data Center Water Efficiency 300x\",  \n  \"description\": \"NVIDIA Blackwell cooling solutions enhance water efficiency by 300x in AI data centers, reducing costs and energy consumption with liquid cooling.\",  \n  \"keywords\": [\"NVIDIA Blackwell cooling\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://blogs.nvidia.com/blog/blackwell-platform-water-efficiency-liquid-cooling-data-centers-ai-factories/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://blogs.nvidia.com/wp-content/uploads/2025/04/water-efficiency-hero-resized.jpg\" },  \n  \"datePublished\": \"2025-04-22T13:00:39Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://i.imgur.com/A5Wdp6f.png\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Chill Factor: NVIDIA Blackwell Platform Boosts Water Efficiency by Over 300x  \n\nAs AI workloads grow in complexity and scale, traditional air cooling methods struggle to keep up with the rising thermal demands of hyperscale data centers. The **NVIDIA Blackwell cooling** platform introduces a breakthrough solution with liquid-cooled systems like the GB200 NVL72 and GB300 NVL72, designed to optimize energy efficiency and heat dissipation. These rack-scale systems deliver up to 300x greater water efficiency compared to conventional air-cooled architectures, addressing one of the biggest operational challenges in AI infrastructure.  \n\nLiquid cooling captures heat directly at the source, eliminating the need for energy-intensive mechanical chillers. By transferring heat through a closed-loop system, data centers can operate with warmer water temperatures, significantly reducing both energy and water consumption. The GB200 NVL72, built on the NVIDIA Blackwell platform, offers 25x greater energy efficiency and 40x higher revenue potential, while the newer GB300 NVL72 improves throughput by 35x. These advancements translate to millions in annual savings for large-scale facilities, with hyperscale data centers potentially cutting cooling-related costs by up to $4 million per year.  \n\n### Industry-Wide Adoption of Liquid Cooling  \nLeading companies like Vertiv, Schneider Electric, and CoolIT Systems are already leveraging NVIDIA’s liquid-cooling innovations to enhance efficiency. Vertiv’s reference architecture for the GB200 NVL72 reduces energy use by 25%, while Schneider Electric’s infrastructure supports racks with up to 132 kW power density. AWS has also adopted liquid cooling, achieving a 46% reduction in energy consumption while maintaining high compute performance.  \n\nWith AI pushing computational limits, liquid cooling is becoming essential for sustainable, high-performance data centers. NVIDIA’s advancements, including participation in the Department of Energy’s COOLERCHIPS program, highlight the industry’s shift toward energy-efficient, water-saving cooling solutions."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "NVIDIA Blackwell cooling",
        "liquid-cooled data centers",
        "AI infrastructure efficiency",
        "GB200 NVL72",
        "GB300 NVL72",
        "water efficiency in data centers",
        "energy-efficient cooling solutions",
        "hyperscale data center cooling",
        "Vertiv liquid cooling",
        "Schneider Electric cooling systems"
    ],
    "tags_agent_error": null,
    "trend_score": 10.0,
    "audio_url": null
}