{
    "id": "fa54918a862e5a23abcd05dbc0223aa56dee34c5a63f0c351c26f30495606734",
    "title": "Customize Amazon Nova models to improve tool usage",
    "link": "https://aws.amazon.com/blogs/machine-learning/customize-amazon-nova-models-to-improve-tool-usage/",
    "published_iso": "2025-04-28T17:47:59Z",
    "summary": "<p>Modern large language models (LLMs) excel in language processing but are limited by their static training data. However, as industries require more adaptive, decision-making AI, integrating tools and external APIs has become essential. This has led to the evolution and rapid rise of agentic workflows, where AI systems autonomously plan, execute, and refine tasks. Accurate tool use is foundational for enhancing the decision-making and operational efficiency of these autonomous agents and building successful and complex agentic workflows.</p> \n<p>In this post, we dissect the technical mechanisms of tool calling using <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html\" rel=\"noopener\" target=\"_blank\">Amazon Nova</a> models through <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock</a>, alongside methods for model customization to refine tool calling precision.</p> \n<h2>Expanding LLM capabilities with tool use</h2> \n<p>LLMs excel at natural language tasks but become significantly more powerful with tool integration, such as APIs and computational frameworks. Tools enable LLMs to access real-time data, perform domain-specific computations, and retrieve precise information, enhancing their reliability and versatility. For example, integrating a weather API allows for accurate, real-time forecasts, or a Wikipedia API provides up-to-date information for complex queries. In scientific contexts, tools like calculators or symbolic engines address numerical inaccuracies in LLMs. These integrations transform LLMs into robust, domain-aware systems capable of handling dynamic, specialized tasks with real-world utility.</p> \n<h2>Amazon Nova models and Amazon Bedrock</h2> \n<p><a href=\"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html\" rel=\"noopener\" target=\"_blank\">Amazon Nova models</a>, unveiled at AWS re:Invent in December 2024, are optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key text-understanding benchmarks at low cost. The series comprises three variants: Micro (text-only, ultra-efficient for edge use), Lite (multimodal, balanced for versatility), and Pro (multimodal, high-performance for complex tasks).</p> \n<p>Amazon Nova models can be used for variety of tasks, from generation to developing agentic workflows. As such, these models have the capability to interface with external tools or services and use them through <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/tool-use.html\" rel=\"noopener\" target=\"_blank\">tool calling</a>. This can be achieved through the Amazon Bedrock console (see <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/getting-started-console.html\" rel=\"noopener\" target=\"_blank\">Getting started with Amazon Nova in the Amazon Bedrock console</a>) and APIs such as <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html\" rel=\"noopener\" target=\"_blank\">Converse</a> and <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html\" rel=\"noopener\" target=\"_blank\">Invoke</a>.</p> \n<p>In addition to using the pre-trained models, developers have the option to fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service custom fine-tuning and <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/customize-distill.html\" rel=\"noopener\" target=\"_blank\">distillation</a> of larger models to smaller ones using the Amazon Bedrock console and APIs.</p> \n<h2>Solution overview</h2> \n<p>The following diagram illustrates the solution architecture.</p> \n<p><img alt=\"Our solution consists data preparation for tool use, finetuning with prepared dataset, hosting the finetuned model and evaluation of the finetuned model\" class=\"alignnone size-full wp-image-104973\" height=\"651\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/iML-18065-SolutionOverviewjpg.jpg\" width=\"941\" /></p> \n<p>For this post, we first prepared a custom dataset for tool usage. We used the test set to evaluate Amazon Nova models through Amazon Bedrock using the Converse and Invoke APIs. We then fine-tuned Amazon Nova Micro and Amazon Nova Lite models through Amazon Bedrock with our fine-tuning dataset. After the fine-tuning process was complete, we evaluated these customized models through provisioned throughput. In the following sections, we go through these steps in more detail.</p> \n<h3>Tools</h3> \n<p>Tool usage in LLMs involves two critical operations: tool selection and argument extraction or generation. For instance, consider a tool designed to retrieve weather information for a specific location. When presented with a query such as “What’s the weather in Alexandria, VA?”, the LLM evaluates its repertoire of tools to determine whether an appropriate tool is available. Upon identifying a suitable tool, the model selects it and extracts the required arguments—here, “Alexandria” and “VA” as structured data types (for example, strings)—to construct the tool call.</p> \n<p>Each tool is rigorously defined with a formal specification that outlines its intended functionality, the mandatory or optional arguments, and the associated data types. Such precise definitions, known as <em>tool config</em>, make sure that tool calls are executed correctly and that argument parsing aligns with the tool’s operational requirements. Following this requirement, the dataset used for this example defines eight tools with their arguments and configures them in a structured JSON format. We define the following eight tools (we use seven of them for fine-tuning and hold out the weather_api_call tool during testing in order to evaluate the accuracy on unseen tool use):</p> \n<ul> \n <li><strong>weather_api_call</strong> – Custom tool for getting weather information</li> \n <li><strong>stat_pull</strong> – Custom tool for identifying stats</li> \n <li><strong>text_to_sql</strong> – Custom text-to-SQL tool</li> \n <li><strong>terminal </strong>– Tool for executing scripts in a terminal</li> \n <li><strong>wikipidea</strong> – Wikipedia API tool to search through Wikipedia pages</li> \n <li><strong>duckduckgo_results_json</strong> – Internet search tool that executes a DuckDuckGo search</li> \n <li><strong>youtube_search </strong>– YouTube API search tool that searches video listings</li> \n <li><strong>pubmed_search </strong>– PubMed search tool that searches PubMed abstracts</li> \n</ul> \n<p>The following code is an example of what a <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/tool-use-definition.html\" rel=\"noopener\" target=\"_blank\">tool configuration</a> for terminal might look like:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{'toolSpec': {'name': 'terminal',\n'description': 'Run shell commands on this MacOS machine ',\n'inputSchema': {'json': {'type': 'object',\n'properties': {'commands': {'type': 'string',\n'description': 'List of shell commands to run. Deserialized using json.loads'}},\n'required': ['commands']}}}},</code></pre> \n</div> \n<h3>Dataset</h3> \n<p>The dataset is a synthetic tool calling dataset created with assistance from a foundation model (FM) from Amazon Bedrock and manually validated and adjusted. This dataset was created for our set of eight tools as discussed in the previous section, with the goal of creating a diverse set of questions and tool invocations that allow another model to learn from these examples and generalize to unseen tool invocations.</p> \n<p>Each entry in the dataset is structured as a JSON object with key-value pairs that define the question (a natural language user query for the model), the ground truth tool required to answer the user query, its arguments (dictionary containing the parameters required to execute the tool), and additional constraints like <code>order_matters: boolean</code>, indicating if argument order is critical, and <code>arg_pattern: optional</code>, a regular expression (regex) for argument validation or formatting. Later in this post, we use these ground truth labels to supervise the training of pre-trained Amazon Nova models, adapting them for tool use. This process, known as <em>supervised fine-tuning</em>, will be explored in detail in the following sections.</p> \n<p>The size of the training set is 560 questions and the test set is 120 questions. The test set consists of 15 questions per tool category, totaling 120 questions. The following are some examples from the dataset:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{\n\"question\": \"Explain the process of photosynthesis\",\n\"answer\": \"wikipedia\",\n\"args\": {'query': 'process of photosynthesis'\n},\n\"order_matters\":False,\n\"arg_pattern\":None\n}\n{\n\"question\": \"Display system date and time\",\n\"answer\": \"terminal\",\n\"args\": {'commands': ['date'\n]\n},\n\"order_matters\":True,\n\"arg_pattern\":None\n}\n{\n\"question\": \"Upgrade the requests library using pip\",\n\"answer\": \"terminal\",\n\"args\": {'commands': ['pip install --upgrade requests'\n]\n},\n\"order_matters\":True,\n\"arg_pattern\": [r'pip(3?) install --upgrade requests'\n]\n}</code></pre> \n</div> \n<h2>Prepare the dataset for Amazon Nova</h2> \n<p>To use this dataset with Amazon Nova models, we need to additionally <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/customize-fine-tune-prepare.html\" rel=\"noopener\" target=\"_blank\">format the data</a> based on a particular chat template. Native tool calling has a translation layer that formats the inputs to the appropriate format before passing the model. Here, we employ a DIY tool use approach with a custom prompt template. Specifically, we need to add the system prompt, the user message embedded with the tool config, and the ground truth labels as the assistant message. The following is a training example formatted for Amazon Nova. Due to space constraints, we only show the toolspec for one tool.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{\"system\": [{\"text\": \"You are a bot that can handle different requests\nwith tools.\"}],\n\"messages\": [{\"role\": \"user\",\n\"content\": [{\"text\": \"Given the following functions within &lt;tools&gt;,\nplease respond with a JSON for a function call with its proper arguments\nthat best answers the given prompt.\n\nRespond in the format\n{\\\"name\\\": function name,\\\"parameters\\\": dictionary of argument name and\nits value}.\nDo not use variables. Donot give any explanations.\n\nONLY output the resulting\nJSON structure and nothing else.\n\nDonot use the word 'json' anywhere in the\nresult.\n&lt;tools&gt;\n&nbsp; &nbsp; {\\\"tools\\\": [{\\\"toolSpec\\\":{\\\"name\\\":\\\"youtube_search\\\",\n&nbsp; &nbsp; \\\"description\\\": \\\" search for youtube videos associated with a person.\n&nbsp; &nbsp; the input to this tool should be a comma separated list, the first part\n&nbsp; &nbsp; contains a person name and the second a number that is the maximum number\n&nbsp; &nbsp; of video results to return aka num_results. the second part is optional\\\",&nbsp;\n&nbsp; &nbsp; \\\"inputSchema\\\":\n&nbsp; &nbsp; {\\\"json\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\": {\\\"query\\\":\n&nbsp; &nbsp; {\\\"type\\\": \\\"string\\\",\n&nbsp; &nbsp; &nbsp;\\\"description\\\": \\\"youtube search query to look up\\\"}},\n&nbsp; &nbsp; \\\"required\\\": [\\\"query\\\"]}}}},]}\n&lt;/tools&gt;\nGenerate answer for the following question.\n&lt;question&gt;\nList any products that have received consistently negative reviews\n&lt;/question&gt;\"}]},\n{\"role\": \"assistant\", \"content\": [{\"text\": \"{'name':text_to_sql,'parameters':\n{'table': 'product_reviews','condition':\n'GROUP BY product_id HAVING AVG(rating) &lt; 2'}}\"}]}],\n\"schemaVersion\": \"tooluse-dataset-2024\"}</code></pre> \n</div> \n<h2>Upload dataset to Amazon S3</h2> \n<p>This step is needed later for the fine-tuning for Amazon Bedrock to access the training data. You can upload your dataset either through the <a href=\"http://aws.amazon.com/s3\" rel=\"noopener\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) console or through code.</p> \n<h2>Tool calling with base models through the Amazon Bedrock API</h2> \n<p>Now that we have created the tool use dataset and formatted it as required, let’s use it to test out the Amazon Nova models. As mentioned previously, we can use both the Converse and Invoke APIs for tool use in Amazon Bedrock. The Converse API enables dynamic, context-aware conversations, allowing models to engage in multi-turn dialogues, and the Invoke API allows the user to call and interact with the underlying models within Amazon Bedrock.</p> \n<p>To use the Converse API, you simply send the messages, system prompt (if any), and the tool config directly in the Converse API. See the following example code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">response = bedrock_runtime.converse(\nmodelId=model_id,\nmessages=messages,\nsystem=system_prompt,\ntoolConfig=tool_config,\n)</code></pre> \n</div> \n<p>To parse the tool and arguments from the LLM response, you can use the following example code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">for content_block in response['output']['message'][\"content\"]:\n\nif \"toolUse\" in content_block:\nout_tool_name=content_block['toolUse']['name']\nout_tool_inputs_dict=content_block['toolUse']['input']\nprint(out_tool_name,out_tool_inputs_dict.keys())</code></pre> \n</div> \n<p>For the question: <code>“Hey, what's the temperature in Paris right now?”</code>, you get the following output:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">weather_api_call dict_keys(['country', 'city'])</code></pre> \n</div> \n<p>To execute tool use through the Invoke API, first you need to prepare the request body with the user question as well as the tool config that was prepared before. The following code snippet shows how to convert the tool config JSON to string format, which can be used in the message body:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\"># Convert tools configuration to JSON string\nformatted_tool_config = json.dumps(tool_config, indent=2)\nprompt = prompt_template.replace(\"{question}\", question)\nprompt = prompt.replace(\"{tool_config}\", formatted_tool_config)\n# message template\nmessages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n# Prepare request body\nmodel_kwargs = {\"system\":system_prompt, \"messages\": messages, \"inferenceConfig\": inferenceConfig,} body = json.dumps(model_kwargs)\nresponse = bedrock_runtime.invoke_model(\nbody=body,\nmodelId=model_id,\naccept=accept,\ncontentType=contentType\n)</code></pre> \n</div> \n<p>Using either of the two APIs, you can test and benchmark the base Amazon Nova models with the tool use dataset. In the next sections, we show how you can customize these base models specifically for the tool use domain.</p> \n<h2>Supervised fine-tuning using the Amazon Bedrock console</h2> \n<p>Amazon Bedrock offers three different <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/customization.html\" rel=\"noopener\" target=\"_blank\">customization</a> techniques: <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/customize-fine-tune.html\" rel=\"noopener\" target=\"_blank\">supervised fine-tuning</a>, <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/customize-distill.html\" rel=\"noopener\" target=\"_blank\">model distillation</a>, and <a href=\"https://aws.amazon.com/blogs/aws/customize-models-in-amazon-bedrock-with-your-own-data-using-fine-tuning-and-continued-pre-training/\" rel=\"noopener\" target=\"_blank\">continued pre-training</a>. At the time of writing, the first two methods are available for customizing Amazon Nova models. Supervised fine-tuning is a popular method in transfer learning, where a pre-trained model is adapted to a specific task or domain by training it further on a smaller, task-specific dataset. The process uses the representations learned during pre-training on large datasets to improve performance in the new domain. During fine-tuning, the model’s parameters (either all or selected layers) are updated using backpropagation to minimize the loss.</p> \n<p>In this post, we use the labeled datasets that we created and formatted previously to run supervised fine-tuning to adapt Amazon Nova models for the tool use domain.</p> \n<h2>Create a fine-tuning job</h2> \n<p>Complete the following steps to create a fine-tuning job:</p> \n<ol> \n <li>Open the Amazon Bedrock console.</li> \n <li>Choose <code>us-east-1</code> as the AWS Region.</li> \n <li>Under <strong>Foundation models</strong> in the navigation pane, choose <strong>Custom models</strong>.</li> \n <li>Choose <strong>Create Fine-tuning job</strong> under <strong>Customization methods</strong>.<strong>&nbsp;</strong></li> \n</ol> \n<p>At the time of writing, Amazon Nova model fine-tuning is exclusively available in the us-east-1 Region.</p> \n<p><img alt=\"create finetuning job from console\" class=\"alignnone size-full wp-image-104974\" height=\"526\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot1.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"975\" /></p> \n<ol start=\"5\"> \n <li>Choose <strong>Select model </strong>and choose <strong>Amazon </strong>as the model provider.</li> \n <li>Choose your model (for this post, Amazon Nova Micro) and choose <strong>Apply</strong>.</li> \n</ol> \n<p><img alt=\"choose model for finetuning\" class=\"alignnone size-full wp-image-104975\" height=\"742\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot2.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1344\" /></p> \n<ol start=\"7\"> \n <li>For <strong>Fine-tuned model name</strong>, enter a unique name.</li> \n <li>For <strong>Job name</strong>¸ enter a name for the fine-tuning job.</li> \n <li>In the <strong>Input data </strong>section, enter following details: \n  <ul> \n   <li>For <strong>S3 location</strong>, enter the source S3 bucket containing the training data.</li> \n   <li>For <strong>Validation dataset location</strong>, optionally enter the S3 bucket containing a validation dataset.</li> \n  </ul> </li> \n</ol> \n<p><img alt=\"Choosing data location in console for finetuning\" class=\"alignnone size-full wp-image-104976\" height=\"833\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot3.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1312\" /></p> \n<ol start=\"10\"> \n <li>In the <strong>Hyperparameters</strong> section, you can customize the following <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html\" rel=\"noopener\" target=\"_blank\">hyperparameters</a>: \n  <ul> \n   <li>For <strong>Epochs</strong>¸ enter a value between 1–5.</li> \n   <li>For <strong>Batch size</strong>, the value is fixed at 1.</li> \n   <li>For <strong>Learning rate multiplier</strong>, enter a value between 0.000001–0.0001</li> \n   <li>For <strong>Learning rate warmup steps</strong>, enter a value between 0–100.</li> \n  </ul> </li> \n</ol> \n<p>We recommend starting with the default parameter values and then changing the settings iteratively. It’s a good practice to change only one or a couple of parameters at a time, in order to isolate the parameter effects. Remember, hyperparameter tuning is model and use case specific.</p> \n<ol start=\"11\"> \n <li>In the<strong> Output data </strong>section, enter the target S3 bucket for model outputs and training metrics.</li> \n <li>Choose <strong>Create fine-tuning job</strong>.</li> \n</ol> \n<h2>Run the fine-tuning job</h2> \n<p>After you start the fine-tuning job, you will be able to see your job under <strong>Jobs </strong>and the status as <strong>Training</strong>. When it finishes, the status changes to <strong>Complete</strong>.</p> \n<p><img alt=\"tracking finetuning job progress\" class=\"alignnone wp-image-104977 size-full\" height=\"447\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot4.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1352\" /></p> \n<p>You can now go to the training job and optionally access the training-related artifacts that are saved in the output folder.</p> \n<p><img alt=\"Check training artifacts\" class=\"alignnone size-full wp-image-104978\" height=\"537\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot5.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1332\" /></p> \n<p>You can find both training and validation (we highly recommend using a validation set) artifacts here.</p> \n<p><img alt=\"training and validation artifacts\" class=\"alignnone size-full wp-image-104979\" height=\"285\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Consolescreenshot6.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1314\" /></p> \n<p>You can use the training and validation artifacts to assess your fine-tuning job through loss curves (as shown in the following figure), which track training loss (orange) and validation loss (blue) over time. A steady decline in both indicates effective learning and good generalization. A small gap between them suggests minimal overfitting, whereas a rising validation loss with decreasing training loss signals overfitting. If both losses remain high, it indicates underfitting. Monitoring these curves helps you quickly diagnose model performance and adjust training strategies for optimal results.</p> \n<p><img alt=\"training and validation loss curves gives insight on how the training progresses\" class=\"alignnone size-full wp-image-104983\" height=\"1101\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065LossCurves.jpg\" width=\"1697\" /></p> \n<h2>Host the fine-tuned model and run inference</h2> \n<p>Now that you have completed the fine-tuning, you can host the model and use it for inference. Follow these steps:</p> \n<ol> \n <li>On the Amazon Bedrock console, under <strong>Foundation models </strong>in the navigation pane, choose <strong>Custom models</strong></li> \n <li>On the <strong>Models </strong>tab, choose the model you fine-tuned.</li> \n</ol> \n<p><img alt=\"starting provisioned throughtput through console to host FT model\" class=\"alignnone size-full wp-image-104984\" height=\"541\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML18065-PTConsolescreenshot1.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1337\" /></p> \n<ol start=\"3\"> \n <li>Choose <strong>Purchase provisioned throughput</strong>.</li> \n</ol> \n<p><img alt=\"start provisioned throughput\" class=\"alignnone size-full wp-image-104980\" height=\"205\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-PTConsolescreenshot2.jpg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1334\" /></p> \n<ol start=\"4\"> \n <li>Specify a commitment term (no commitment, 1 month, 6 months) and review the associated cost for hosting the fine-tuned models.</li> \n</ol> \n<p>After the customized model is hosted through provisioned throughput, a model ID will be assigned, which will be used for inference. For inference with models hosted with provisioned throughput, we have to use the Invoke API in the same way we described previously in this post—simply replace the model ID with the customized model ID.</p> \n<p>The aforementioned fine-tuning and inference steps can also be done programmatically. Refer to the following <a href=\"https://github.com/aws-samples/amazon-nova-samples\" rel=\"noopener\" target=\"_blank\">GitHub repo</a> for more detail.</p> \n<h2>Evaluation framework</h2> \n<p>Evaluating fine-tuned tool calling LLMs requires a comprehensive approach to assess their performance across various dimensions. The primary metric to evaluate tool calling is accuracy, including both tool selection and argument generation accuracy. This measures how effectively the model selects the correct tool and generates valid arguments. Latency and token usage (input and output tokens) are two other important metrics.</p> \n<p>Tool call accuracy evaluates if the tool predicted by the LLM matches the ground truth tool for each question; a score of 1 is given if they match and 0 when they don’t. After processing the questions, we can use the following equation: <code>Tool Call Accuracy=∑(Correct Tool Calls)/(Total number of test questions)</code>.</p> \n<p>Argument call accuracy assesses whether the arguments provided to the tools are correct, based on either exact matches or regex pattern matching. For each tool call, the model’s predicted arguments are extracted. It uses the following argument matching methods:</p> \n<ul> \n <li><strong>Regex matching</strong> – If the ground truth includes regex patterns, the predicted arguments are matched against these patterns. A successful match increases the score.</li> \n <li><strong>Inclusive string matching </strong>– If no regex pattern is provided, the predicted argument is compared to the ground truth argument. Credit is given if the predicted argument contains the ground truth argument. This is to allow for arguments, like search terms, to not be penalized for adding additional specificity.</li> \n</ul> \n<p>The score for each argument is normalized based on the number of arguments, allowing partial credit when multiple arguments are required. The cumulative correct argument scores are averaged across all questions: <code>Argument Call Accuracy = ∑Correct Arguments/(Total Number of Questions)</code>.</p> \n<p>Below we show some example questions and accuracy scores:</p> \n<p><strong>Example&nbsp;1:</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">User question: Execute this run.py script with an argparse arg adding two gpus\nGT tool: terminal &nbsp; LLM output tool: terminal\nPred args: &nbsp;['python run.py —gpus 2']\nGround truth pattern: python(3?) run.py —gpus 2\nArg matching method: regex match\nArg matching score: 1.0</code></pre> \n</div> \n<p><strong>Example&nbsp;2:</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">User question: Who had the most rushing touchdowns for the bengals in 2017 season?\nGT tool: stat_pull &nbsp; LLM output tool: stat_pull\nPred args: &nbsp;['NFL']\nstraight match\narg score 0.3333333333333333\nPred args: &nbsp;['2017']\nStraight match\nArg score 0.6666666666666666\nPred args: &nbsp;['Cincinnati Bengals']\nStraight match\nArg score 1.0</code></pre> \n</div> \n<h2>Results</h2> \n<p>We are now ready to visualize the results and compare the performance of base Amazon Nova models to their fine-tuned counterparts.</p> \n<h3>Base models</h3> \n<p>The following figures illustrate the performance comparison of the base Amazon Nova models.</p> \n<p><img alt=\"Improvement of finetuned models over base models in tool use \" class=\"alignnone size-full wp-image-104981\" height=\"543\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Result1.jpg\" width=\"1287\" /></p> \n<p>The comparison reveals a clear trade-off between accuracy and latency, shaped by model size. Amazon Nova Pro, the largest model, delivers the highest accuracy in both tool call and argument call tasks, reflecting its advanced computational capabilities. However, this comes with increased latency.</p> \n<p>In contrast, Amazon Nova Micro, the smallest model, achieves the lowest latency, which ideal for fast, resource-constrained environments, though it sacrifices some accuracy compared to its larger counterparts.</p> \n<h3>Fine-tuned models vs. base models</h3> \n<p>The following figure visualizes accuracy improvement after fine-tuning.</p> \n<p><img alt=\"Improvement of finetuned models over base models in tool use \" class=\"alignnone size-full wp-image-104982\" height=\"837\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/ML-18065-Result2.jpg\" width=\"1288\" /></p> \n<p>The comparative analysis of the Amazon Nova model variants reveals substantial performance improvements through fine-tuning, with the most significant gains observed in the smaller Amazon Nova Micro model. The fine-tuned Amazon Nova model showed remarkable growth in tool call accuracy, increasing from 75.8% to 95%, which is a 25.38% improvement. Similarly, its argument call accuracy rose from 77.8% to 87.7%, reflecting a 12.74% increase.</p> \n<p>In contrast, the fine-tuned Amazon Nova Lite model exhibited more modest gains, with tool call accuracy improving from 90.8% to 96.66%—a 6.46% increase—and argument call accuracy rising from 85% to 89.9%, marking a 5.76% improvement. Both fine-tuned models surpassed the accuracy achieved by the Amazon Nova Pro base model.</p> \n<p>These results highlight that fine-tuning can significantly enhance the performance of lightweight models, making them strong contenders for applications where both accuracy and latency are critical.</p> \n<h2>Conclusion</h2> \n<p>In this post, we demonstrated model customization (fine-tuning) for tool use with Amazon Nova. We first introduced a tool usage use case, and gave details about the dataset. We walked through the details of Amazon Nova specific data formatting and showed how to do tool calling through the Converse and Invoke APIs in Amazon Bedrock. After getting the baseline results from Amazon Nova models, we explained in detail the fine-tuning process, hosting fine-tuned models with provisioned throughput, and using the fine-tuned Amazon Nova models for inference. In addition, we touched upon getting insights from training and validation artifacts from a fine-tuning job in Amazon Bedrock.</p> \n<p>Check out the <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/custom-models/bedrock-fine-tuning/nova/understanding/nova_tooluse_customization\" rel=\"noopener\" target=\"_blank\">detailed notebook</a> for tool usage to learn more. For more information on Amazon Bedrock and the latest Amazon Nova models, refer to the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock User Guide</a> and <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html\" rel=\"noopener\" target=\"_blank\">Amazon Nova User Guide</a>. The Generative AI Innovation Center has a group of AWS science and strategy experts with comprehensive expertise spanning the generative AI journey, helping customers prioritize use cases, build roadmaps, and move solutions into production. See <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" rel=\"noopener\" target=\"_blank\">Generative AI Innovation Center</a> for our latest work and customer success stories.</p> \n<hr /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-105129 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/24/baishali-1.jpeg\" width=\"100\" /><strong>Baishali Chaudhury</strong> is an Applied Scientist at the Generative AI Innovation Center at AWS, where she focuses on advancing Generative AI solutions for real-world applications. She has a strong background in computer vision, machine learning, and AI for healthcare. Baishali holds a PhD in Computer Science from University of South Florida and PostDoc from Moffitt Cancer Centre.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-104987 alignleft\" height=\"137\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/Isaac-Privitera.jpg\" width=\"100\" /><strong>Isaac Privitera</strong>&nbsp;is a Principal Data Scientist with the AWS Generative AI Innovation Center, where he develops bespoke generative AI-based solutions to address customers’ business problems. His primary focus lies in building responsible AI systems, using techniques such as RAG, multi-agent systems, and model fine-tuning. When not immersed in the world of AI, Isaac can be found on the golf course, enjoying a football game, or hiking trails with his loyal canine companion, Barry.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft wp-image-104986 size-full\" height=\"140\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/florawan.jpg\" width=\"100\" />Mengdie (Flora) Wang</strong> is a Data Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalableGenerative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology. Prior to AWS, Flora earned her Master’s degree in Computer Science from the University of Minnesota, where she developed her expertise in machine learning and artificial intelligence.</p>",
    "source_feed": "https://aws.amazon.com/blogs/machine-learning/feed/",
    "scraped_at_iso": "2025-05-02T20:50:44Z",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "AI Models",
        "reasoning_summary": "The article discusses the customization of Amazon Nova models for improved tool usage, presenting verifiable technical details and methods for enhancing tool calling precision, which is relevant and significant in the AI field.",
        "primary_topic_keyword": "Amazon Nova models"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-02T20:55:04Z",
    "topic": "AI Models",
    "is_breaking": false,
    "primary_keyword": "Amazon Nova models",
    "selected_image_url": "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/iML-18065-SolutionOverviewjpg-911x630.jpg",
    "seo_agent_results": {
        "generated_title_tag": "Customize Amazon Nova Models for Enhanced Tool Usage",
        "generated_meta_description": "Learn how Amazon Nova models can be fine-tuned to improve tool integration and decision-making in AI workflows.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"Customize Amazon Nova Models for Enhanced Tool Usage\",  \n  \"description\": \"Learn how Amazon Nova models can be fine-tuned to improve tool integration and decision-making in AI workflows.\",  \n  \"keywords\": [\"Amazon Nova models\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://aws.amazon.com/blogs/machine-learning/customize-amazon-nova-models-to-improve-tool-usage/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/23/iML-18065-SolutionOverviewjpg-911x630.jpg\" },  \n  \"datePublished\": \"2025-04-28T17:47:59Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://i.imgur.com/A5Wdp6f.png\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Customize Amazon Nova Models to Improve Tool Usage  \n\nModern large language models (LLMs) excel in language processing but face limitations due to static training data. To address this, industries are increasingly adopting agentic workflows, where AI systems autonomously plan, execute, and refine tasks using external tools and APIs. **Amazon Nova models**, available through Amazon Bedrock, are optimized for such workflows, offering state-of-the-art performance while maintaining cost efficiency. These models can be fine-tuned to enhance tool-calling precision, making them ideal for dynamic, real-world applications.  \n\nAmazon Nova models come in three variants: Micro (text-only, ultra-efficient), Lite (multimodal, balanced), and Pro (multimodal, high-performance). Developers can customize these models using supervised fine-tuning, distillation, or continued pre-training to improve accuracy, latency, and cost-effectiveness. For example, integrating APIs like weather services or Wikipedia allows Nova models to retrieve real-time data, significantly expanding their utility in specialized domains.  \n\n### Fine-Tuning for Tool Use  \n\nA key advantage of **Amazon Nova models** is their ability to interface with external tools through Amazon Bedrock’s Converse and Invoke APIs. Fine-tuning involves preparing a labeled dataset, adjusting hyperparameters, and evaluating performance using metrics like tool call accuracy and argument extraction precision. Results show that fine-tuned Nova Micro models achieve up to 25% improvement in tool selection accuracy, making them competitive with larger base models while maintaining lower latency."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "Amazon Nova models",
        "Amazon Bedrock",
        "large language models",
        "agentic workflows",
        "AI fine-tuning",
        "tool-calling precision",
        "multimodal AI",
        "Converse and Invoke APIs",
        "real-time data integration"
    ],
    "tags_agent_error": null,
    "trend_score": 9.5,
    "slug": "customize-amazon-nova-models-to-improve-tool-usage",
    "audio_url": null
}