{
    "id": "a01e04eaae8ba1599d49f8dd7b32e1c1891729043a315b08599421083a5b9d65",
    "title": "Use custom metrics to evaluate your generative AI application with Amazon Bedrock",
    "link": "https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/",
    "published_iso": "2025-05-06T21:39:21Z",
    "summary": "<p>With <a href=\"https://aws.amazon.com/bedrock/evaluations/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock Evaluations</a>, you can evaluate foundation models (FMs) and Retrieval Augmented Generation (RAG) systems, whether hosted on <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock</a> or another model or RAG system hosted elsewhere, including <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock Knowledge Bases</a> or multi-cloud and on-premises deployments. We <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-models-or-rag-systems-using-amazon-bedrock-evaluations-now-generally-available/\" rel=\"noopener\" target=\"_blank\">recently announced the general availability</a> of the large language model (LLM)-as-a-judge technique in model evaluation and the new RAG evaluation tool, also powered by an LLM-as-a-judge behind the scenes. These tools are already empowering organizations to systematically evaluate FMs and RAG systems with enterprise-grade tools. We also mentioned that these evaluation tools don’t have to be limited to models or RAG systems hosted on Amazon Bedrock; with the bring your own inference (BYOI) responses feature, you can evaluate models or applications if you use the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-judge.html\" rel=\"noopener\" target=\"_blank\">input formatting requirements</a> for either offering.</p> \n<p>The LLM-as-a-judge technique powering these evaluations enables automated, human-like evaluation quality at scale, using FMs to assess quality and responsible AI dimensions without manual intervention. With built-in metrics like correctness (factual accuracy), completeness (response thoroughness), faithfulness (hallucination detection), and responsible AI metrics such as harmfulness and answer refusal, you and your team can evaluate models hosted on Amazon Bedrock and knowledge bases natively, or using BYOI responses from your custom-built systems.</p> \n<p>Amazon Bedrock Evaluations offers an extensive list of built-in metrics for both evaluation tools, but there are times when you might want to define these evaluation metrics in a different way, or make completely new metrics that are relevant to your use case. For example, you might want to define a metric that evaluates an application response’s adherence to your specific brand voice, or want to classify responses according to a custom categorical rubric. You might want to use numerical scoring or categorical scoring for various purposes. For these reasons, you need a way to use custom metrics in your evaluations.</p> \n<p>Now with Amazon Bedrock, you can develop custom evaluation metrics for both model and RAG evaluations. This capability extends the LLM-as-a-judge framework that drives Amazon Bedrock Evaluations.</p> \n<p>In this post, we demonstrate how to use custom metrics in Amazon Bedrock Evaluations to measure and improve the performance of your generative AI applications according to your specific business requirements and evaluation criteria.</p> \n<h2>Overview</h2> \n<p>Custom metrics in Amazon Bedrock Evaluations offer the following features:</p> \n<ul> \n <li><strong>Simplified getting started experience</strong> – Pre-built starter templates are available on the <a href=\"http://aws.amazon.com/console\" rel=\"noopener\" target=\"_blank\">AWS Management Console</a> based on our industry-tested built-in metrics, with options to create from scratch for specific evaluation criteria.</li> \n <li><strong>Flexible scoring systems</strong> – Support is available for both quantitative (numerical) and qualitative (categorical) scoring to create ordinal metrics, nominal metrics, or even use evaluation tools for classification tasks.</li> \n <li><strong>Streamlined workflow management</strong> – You can save custom metrics for reuse across multiple evaluation jobs or import previously defined metrics from JSON files.</li> \n <li><strong>Dynamic content integration</strong> – With built-in template variables (for example, <code>{{prompt}}</code>, <code>{{prediction}}</code>, and <code>{{context}}</code>), you can seamlessly inject dataset content and model outputs into evaluation prompts.</li> \n <li><strong>Customizable output control</strong> – You can use our recommended output schema for consistent results, with advanced options to define custom output formats for specialized use cases.</li> \n</ul> \n<p>Custom metrics give you unprecedented control over how you measure AI system performance, so you can align evaluations with your specific business requirements and use cases. Whether assessing factuality, coherence, helpfulness, or domain-specific criteria, custom metrics in Amazon Bedrock enable more meaningful and actionable evaluation insights.</p> \n<p>In the following sections, we walk through the steps to create a job with model evaluation and custom metrics using both the Amazon Bedrock console and the Python SDK and APIs.</p> \n<h2>Supported data formats</h2> \n<p>In this section, we review some important data formats.</p> \n<h3>Judge prompt uploading</h3> \n<p>To upload your previously saved custom metrics into an evaluation job, follow the JSON format in the following examples.</p> \n<p>The following code illustrates a definition with numerical scale:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"customMetricDefinition\": {\n        \"metricName\": \"my_custom_metric\",\n        \"instructions\": \"Your complete custom metric prompt including at least one {{input variable}}\",\n        \"ratingScale\": [\n            {\n                \"definition\": \"first rating definition\",\n                \"value\": {\n                    \"floatValue\": 3\n                }\n            },\n            {\n                \"definition\": \"second rating definition\",\n                \"value\": {\n                    \"floatValue\": 2\n                }\n            },\n            {\n                \"definition\": \"third rating definition\",\n                \"value\": {\n                    \"floatValue\": 1\n                }\n            }\n        ]\n    }\n}</code></pre> \n</div> \n<p>The following code illustrates a definition with string scale:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"customMetricDefinition\": {\n        \"metricName\": \"my_custom_metric\",\n        \"instructions\": \"Your complete custom metric prompt including at least one {{input variable}}\",\n        \"ratingScale\": [\n            {\n                \"definition\": \"first rating definition\",\n                \"value\": {\n                    \"stringValue\": \"first value\"\n                }\n            },\n            {\n                \"definition\": \"second rating definition\",\n                \"value\": {\n                    \"stringValue\": \"second value\"\n                }\n            },\n            {\n                \"definition\": \"third rating definition\",\n                \"value\": {\n                    \"stringValue\": \"third value\"\n                }\n            }\n        ]\n    }\n}</code></pre> \n</div> \n<p>The following code illustrates a definition with no scale:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"customMetricDefinition\": {\n        \"metricName\": \"my_custom_metric\",\n        \"instructions\": \"Your complete custom metric prompt including at least one {{input variable}}\"\n    }\n}</code></pre> \n</div> \n<p>For more information on defining a judge prompt with no scale, see the best practices section later in this post.</p> \n<h3>Model evaluation dataset format</h3> \n<p>When using LLM-as-a-judge, only one model can be evaluated per evaluation job. Consequently, you must provide a single entry in the <code>modelResponses</code> list for each evaluation, though you can run multiple evaluation jobs to compare different models. The <code>modelResponses</code> field is required for BYOI jobs, but not needed for non-BYOI jobs. The following is the input JSONL format for LLM-as-a-judge in model evaluation. Fields marked with <code>?</code> are optional.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"prompt\": string\n    \"referenceResponse\"?: string\n    \"category\"?: string\n     \"modelResponses\"?: [\n        {\n            \"response\": string\n            \"modelIdentifier\": string\n        }\n    ]\n}</code></pre> \n</div> \n<h3>RAG evaluation dataset format</h3> \n<p>We updated the evaluation job input dataset format to be even more flexible for RAG evaluation. Now, you can bring <code>referenceContexts</code>, which are expected retrieved passages, so you can compare your actual retrieved contexts to your expected retrieved contexts. You can find the new <code>referenceContexts</code> field in the updated JSONL schema for RAG evaluation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"conversationTurns\": [{\n            \"prompt\": {\n                \"content\": [{\n                    \"text\": string\n                }]\n            },\n            \"referenceResponses\": [{\n                \"content\": [{\n                    \"text\": string\n                }]\n            }],\n            \"referenceContexts\" ? : [{\n                \"content\": [{\n                    \"text\": string\n                }]\n            }],\n            \"output\": {\n                \"text\": string \"modelIdentifier\" ? : string \"knowledgeBaseIdentifier\": string \"retrievedPassages\": {\n                    \"retrievalResults\": [{\n                        \"name\" ? : string \"content\": {\n                            \"text\": string\n                        },\n                        \"metadata\" ? : {\n                            [key: string]: string\n                        }\n                    }]\n                }\n            }]\n    }</code></pre> \n</div> \n<h3>Variables for data injection into judge prompts</h3> \n<p>To make sure that your data is injected into the judge prompts in the right place, use the variables from the following table. We have also included a guide to show you where the evaluation tool will pull data from your input file, if applicable. There are cases where if you bring your own inference responses to the evaluation job, we will use that data from your input file; if you don’t use bring your own inference responses, then we will call the Amazon Bedrock model or knowledge base and prepare the responses for you.</p> \n<p>The following table summarizes the variables for model evaluation.</p> \n<table border=\"1px\" cellpadding=\"5px\" style=\"height: 338px;\" width=\"715\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td width=\"87\"><span style=\"color: #ffffff;\"><strong>Plain Name</strong></span></td> \n   <td width=\"122\"><span style=\"color: #ffffff;\"><strong>Variable</strong></span></td> \n   <td width=\"252\"><span style=\"color: #ffffff;\"><strong>Input Dataset JSONL Key</strong></span></td> \n   <td width=\"133\"><span style=\"color: #ffffff;\"><strong>Mandatory or Optional</strong></span></td> \n  </tr> \n  <tr> \n   <td width=\"87\">Prompt</td> \n   <td width=\"122\"><code>{{prompt}}</code></td> \n   <td width=\"252\">prompt</td> \n   <td width=\"133\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"87\">Response</td> \n   <td width=\"122\"><code>{{prediction}}</code></td> \n   <td width=\"252\"> <p>For a BYOI job:</p> <p><code>modelResponses.response </code></p> <p>If you don’t bring your own inference responses, the evaluation job will call the model and prepare this data for you.</p></td> \n   <td width=\"133\">Mandatory</td> \n  </tr> \n  <tr> \n   <td width=\"87\">Ground truth response</td> \n   <td width=\"122\"><code>{{ground_truth}}</code></td> \n   <td width=\"252\"><code>referenceResponse</code></td> \n   <td width=\"133\">Optional</td> \n  </tr> \n </tbody> \n</table> \n<p>The following table summarizes the variables for RAG evaluation (retrieve only).</p> \n<table border=\"1px\" cellpadding=\"5px\" style=\"height: 627px;\" width=\"736\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td width=\"86\"><span style=\"color: #ffffff;\"><strong>Plain Name</strong></span></td> \n   <td width=\"161\"><span style=\"color: #ffffff;\"><strong>Variable</strong></span></td> \n   <td width=\"259\"><span style=\"color: #ffffff;\"><strong>Input Dataset JSONL Key</strong></span></td> \n   <td width=\"89\"><span style=\"color: #ffffff;\"><strong>Mandatory or Optional</strong></span></td> \n  </tr> \n  <tr> \n   <td width=\"86\">Prompt</td> \n   <td width=\"161\"><code>{{prompt}}</code></td> \n   <td width=\"259\"><code>prompt</code></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Ground truth response</td> \n   <td width=\"161\"><code>{{ground_truth}}</code></td> \n   <td width=\"259\"> <p>For a BYOI job:</p> <p><code>output.retrievedResults.retrievalResults </code></p> <p>If you don’t bring your own inference responses, the evaluation job will call the Amazon Bedrock knowledge base and prepare this data for you.</p></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Retrieved passage</td> \n   <td width=\"161\"><code>{{context}}</code></td> \n   <td width=\"259\"> <p>For a BYOI job:</p> <p><code>output.retrievedResults.retrievalResults </code></p> <p>If you don’t bring your own inference responses, the evaluation job will call the Amazon Bedrock knowledge base and prepare this data for you.</p></td> \n   <td width=\"89\">Mandatory</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Ground truth retrieved passage</td> \n   <td width=\"161\"><code>{{reference_contexts}}</code></td> \n   <td width=\"259\"><code>referenceContexts</code></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n </tbody> \n</table> \n<p>The following table summarizes the variables for RAG evaluation (retrieve and generate).</p> \n<table border=\"1px\" cellpadding=\"5px\" style=\"height: 632px;\" width=\"741\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td width=\"86\"><span style=\"color: #ffffff;\"><strong>Plain Name</strong></span></td> \n   <td width=\"161\"><span style=\"color: #ffffff;\"><strong>Variable</strong></span></td> \n   <td width=\"260\"><span style=\"color: #ffffff;\"><strong>Input dataset JSONL key</strong></span></td> \n   <td width=\"89\"><span style=\"color: #ffffff;\"><strong>Mandatory or optional</strong></span></td> \n  </tr> \n  <tr> \n   <td width=\"86\">Prompt</td> \n   <td width=\"161\"><code>{{prompt}}</code></td> \n   <td width=\"260\"><code>prompt</code></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Response</td> \n   <td width=\"161\"><code>{{prediction}}</code></td> \n   <td width=\"260\"> <p>For a BYOI job:</p> <p><code>Output.text</code></p> <p>If you don’t bring your own inference responses, the evaluation job will call the Amazon Bedrock knowledge base and prepare this data for you.</p></td> \n   <td width=\"89\">Mandatory</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Ground truth response</td> \n   <td width=\"161\"><code>{{ground_truth}}</code></td> \n   <td width=\"260\"><code>referenceResponses</code></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Retrieved passage</td> \n   <td width=\"161\"><code>{{context}}</code></td> \n   <td width=\"260\"> <p>For a BYOI job:</p> <p><code>Output.retrievedResults.retrievalResults</code></p> <p>If you don’t bring your own inference responses, the evaluation job will call the Amazon Bedrock knowledge base and prepare this data for you.</p></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n  <tr> \n   <td width=\"86\">Ground truth retrieved passage</td> \n   <td width=\"161\"><code>{{reference_contexts}}</code></td> \n   <td width=\"260\"><code>referenceContexts</code></td> \n   <td width=\"89\">Optional</td> \n  </tr> \n </tbody> \n</table> \n<h2>Prerequisites</h2> \n<p>To use the LLM-as-a-judge model evaluation and RAG evaluation features with BYOI, you must have the following prerequisites:</p> \n<ul> \n <li>AWS account and model access: \n  <ul> \n   <li>An active <a href=\"https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&client_id=signup\" rel=\"noopener\" target=\"_blank\">AWS account</a></li> \n   <li>Selected evaluator and generator models enabled in Amazon Bedrock (verify on the <strong>Model access</strong> page of the Amazon Bedrock console)</li> \n   <li>Confirmed <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\" rel=\"noopener\" target=\"_blank\">AWS Regions</a> where the models are <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html\" rel=\"noopener\" target=\"_blank\">available and their quotas</a></li> \n  </ul> </li> \n <li><a href=\"https://aws.amazon.com/iam/\" rel=\"noopener\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) and <a href=\"http://aws.amazon.com/s3\" rel=\"noopener\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) configuration: \n  <ul> \n   <li>Completed IAM setup and <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html\" rel=\"noopener\" target=\"_blank\">permissions</a> for both model and RAG evaluation</li> \n   <li>Configured <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-language-overview.html\" rel=\"noopener\" target=\"_blank\">S3 bucket</a> with appropriate permissions for accessing and writing output data</li> \n   <li>Enabled <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html\" rel=\"noopener\" target=\"_blank\">CORS</a> on your S3 bucket</li> \n  </ul> </li> \n</ul> \n<h2>Create a model evaluation job with custom metrics using Amazon Bedrock Evaluations</h2> \n<p>Complete the following steps to create a job with model evaluation and custom metrics using Amazon Bedrock Evaluations:</p> \n<ol> \n <li>On the Amazon Bedrock console, choose <strong>Evaluations</strong> in the navigation pane and choose the <strong>Models</strong></li> \n <li>In the <strong>Model evaluation </strong>section, on the <strong>Create</strong> dropdown menu, choose <strong>Automatic: model as a judge</strong>.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105989\" height=\"1036\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image001.jpg\" width=\"1340\" /></li> \n <li>For the <strong>Model evaluation details</strong>, enter an evaluation name and optional description.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105990\" height=\"658\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image003.jpg\" width=\"1289\" /></li> \n <li>For <strong>Evaluator model</strong>, choose the model you want to use for automatic evaluation.</li> \n <li>For <strong>Inference source</strong>, select the source and choose the model you want to evaluate.</li> \n</ol> \n<p>For this example, we chose <strong>Claude 3.5 Sonnet</strong> as the evaluator model, <strong>Bedrock models</strong> as our inference source, and <strong>Claude 3.5 Haiku</strong> as our model to evaluate.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105991\" height=\"1136\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image005.jpg\" width=\"1572\" /></p> \n<ol start=\"5\"> \n <li>The console will display the default metrics for the evaluator model you chose. You can select other metrics as needed.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105992\" height=\"714\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image007.jpg\" width=\"1658\" /></li> \n <li>In the <strong>Custom Metrics</strong> section, we create a new metric called “Comprehensiveness.” Use the template provided and modify based on your metrics. You can use the following variables to define the metric, where only <code>{{prediction}}</code> is mandatory: \n  <ol> \n   <li><code>prompt</code></li> \n   <li><code>prediction</code></li> \n   <li><code>ground_truth</code></li> \n  </ol> </li> \n</ol> \n<p>The following is the metric we defined in full:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">Your role is to judge the comprehensiveness of an answer based on the question and \nthe prediction. Assess the quality, accuracy, and helpfulness of language model response,\n and use these to judge how comprehensive the response is. Award higher scores to responses\n that are detailed and thoughtful.\n\nCarefully evaluate the comprehensiveness of the LLM response for the given query (prompt)\n against all specified criteria. Assign a single overall score that best represents the \ncomprehensivenss, and provide a brief explanation justifying your rating, referencing \nspecific strengths and weaknesses observed.\n\nWhen evaluating the response quality, consider the following rubrics:\n- Accuracy: Factual correctness of information provided\n- Completeness: Coverage of important aspects of the query\n- Clarity: Clear organization and presentation of information\n- Helpfulness: Practical utility of the response to the user\n\nEvaluate the following:\n\nQuery:\n{{prompt}}\n\nResponse to evaluate:\n{{prediction}}</code></pre> \n</div> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105993\" height=\"870\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image009.jpg\" width=\"910\" /></p> \n<ol start=\"7\"> \n <li>Create the output schema and additional metrics. Here, we define a scale that provides maximum points (10) if the response is very comprehensive, and 1 if the response is not comprehensive at all.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105994\" height=\"791\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image011.jpg\" width=\"904\" /></li> \n <li>For <strong>Datasets</strong>, enter your input and output locations in Amazon S3.</li> \n <li>For <strong>Amazon Bedrock IAM role – Permissions</strong>, select <strong>Use an existing service role</strong> and choose a role.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105995\" height=\"1234\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image013.jpg\" width=\"1641\" /></li> \n <li>Choose <strong>Create</strong> and wait for the job to complete.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105996\" height=\"299\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image015.jpg\" width=\"1286\" /></li> \n</ol> \n<h2>Considerations and best practices</h2> \n<p>When using the output schema of the custom metrics, note the following:</p> \n<ul> \n <li>If you use the built-in output schema (recommended), do not add your grading scale into the main judge prompt. The evaluation service will automatically concatenate your judge prompt instructions with your defined output schema rating scale and some structured output instructions (unique to each judge model) behind the scenes. This is so the evaluation service can parse the judge model’s results and display them on the console in graphs and calculate average values of numerical scores.</li> \n <li>The fully concatenated judge prompts are visible in the <strong>Preview</strong> window if you are using the Amazon Bedrock console to construct your custom metrics. Because judge LLMs are inherently stochastic, there might be some responses we can’t parse and display on the console and use in your average score calculations. However, the raw judge responses are always loaded into your S3 output file, even if the evaluation service cannot parse the response score from the judge model.</li> \n <li>If you don’t use the built-in output schema feature (we recommend you use it instead of ignoring it), then you are responsible for providing your rating scale in the judge prompt instructions body. However, the evaluation service will not add structured output instructions and will not parse the results to show graphs; you will see the full judge output plaintext results on the console without graphs and the raw data will still be in your S3 bucket.</li> \n</ul> \n<h2>Create a model evaluation job with custom metrics using the Python SDK and APIs</h2> \n<p>To use the Python SDK to create a model evaluation job with custom metrics, follow these steps (or refer to our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-observe/bedrock-eval-custom-metrics\" rel=\"noopener\" target=\"_blank\">example notebook</a>):</p> \n<ol start=\"2\"> \n <li>Set up the required configurations, which should include your model identifier for the default metrics and custom metrics evaluator, IAM role with appropriate permissions, Amazon S3 paths for input data containing your inference responses, and output location for results: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\">import boto3\nimport time\nfrom datetime import datetime\n\n# Configure knowledge base and model settings\nevaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\ngenerator_model = \"amazon.nova-lite-v1:0\"\ncustom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\nrole_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\nBUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n\n# Specify S3 locations\ninput_data = f\"s3://{BUCKET_NAME}/evaluation_data/input.jsonl\"\noutput_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n\n# Create Bedrock client\n# NOTE: You can change the region name to the region of your choosing.\nbedrock_client = boto3.client('bedrock', region_name='us-east-1') </code></pre> \n  </div> </li> \n <li>To define a custom metric for model evaluation, create a JSON structure with a <code>customMetricDefinition</code> Include your metric’s name, write detailed evaluation instructions incorporating template variables (such as <code>{{prompt}}</code> and <code>{{prediction}}</code>), and define your <code>ratingScale</code> array with assessment values using either numerical scores (<code>floatValue</code>) or categorical labels (<code>stringValue</code>). This properly formatted JSON schema enables Amazon Bedrock to evaluate model outputs consistently according to your specific criteria. \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\">comprehensiveness_metric ={\n    \"customMetricDefinition\": {\n        \"name\": \"comprehensiveness\",\n        \"instructions\": \"\"\"Your role is to judge the comprehensiveness of an \nanswer based on the question and the prediction. Assess the quality, accuracy, \nand helpfulness of language model response, and use these to judge how comprehensive\n the response is. Award higher scores to responses that are detailed and thoughtful.\n\nCarefully evaluate the comprehensiveness of the LLM response for the given query (prompt)\n against all specified criteria. Assign a single overall score that best represents the \ncomprehensivenss, and provide a brief explanation justifying your rating, referencing \nspecific strengths and weaknesses observed.\n\nWhen evaluating the response quality, consider the following rubrics:\n- Accuracy: Factual correctness of information provided\n- Completeness: Coverage of important aspects of the query\n- Clarity: Clear organization and presentation of information\n- Helpfulness: Practical utility of the response to the user\n\nEvaluate the following:\n\nQuery:\n{{prompt}}\n\nResponse to evaluate:\n{{prediction}}\"\"\",\n        \"ratingScale\": [\n            {\n                \"definition\": \"Very comprehensive\",\n                \"value\": {\n                    \"floatValue\": 10\n                }\n            },\n            {\n                \"definition\": \"Mildly comprehensive\",\n                \"value\": {\n                    \"floatValue\": 3\n                }\n            },\n            {\n                \"definition\": \"Not at all comprehensive\",\n                \"value\": {\n                    \"floatValue\": 1\n                }\n            }\n        ]\n    }\n}</code></pre> \n  </div> </li> \n <li>To create a model evaluation job with custom metrics, use the <code>create_evaluation_job</code> API and include your custom metric in the <code>customMetricConfig</code> section, specifying both built-in metrics (such as <code>Builtin.Correctness</code>) and your custom metric in the <code>metricNames</code> array. Configure the job with your generator model, evaluator model, and proper Amazon S3 paths for input dataset and output results. \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\"># Create the model evaluation job\nmodel_eval_job_name = f\"model-evaluation-custom-metrics{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\nmodel_eval_job = bedrock_client.create_evaluation_job(\n    jobName=model_eval_job_name,\n    jobDescription=\"Evaluate model performance with custom comprehensiveness metric\",\n    roleArn=role_arn,\n    applicationType=\"ModelEvaluation\",\n    inferenceConfig={\n        \"models\": [{\n            \"bedrockModel\": {\n                \"modelIdentifier\": generator_model\n            }\n        }]\n    },\n    outputDataConfig={\n        \"s3Uri\": output_path\n    },\n    evaluationConfig={\n        \"automated\": {\n            \"datasetMetricConfigs\": [{\n                \"taskType\": \"General\",\n                \"dataset\": {\n                    \"name\": \"ModelEvalDataset\",\n                    \"datasetLocation\": {\n                        \"s3Uri\": input_data\n                    }\n                },\n                \"metricNames\": [\n                    \"Builtin.Correctness\",\n                    \"Builtin.Completeness\",\n                    \"Builtin.Coherence\",\n                    \"Builtin.Relevance\",\n                    \"Builtin.FollowingInstructions\",\n                    \"comprehensiveness\"\n                ]\n            }],\n            \"customMetricConfig\": {\n                \"customMetrics\": [\n                    comprehensiveness_metric\n                ],\n                \"evaluatorModelConfig\": {\n                    \"bedrockEvaluatorModels\": [{\n                        \"modelIdentifier\": custom_metrics_evaluator_model\n                    }]\n                }\n            },\n            \"evaluatorModelConfig\": {\n                \"bedrockEvaluatorModels\": [{\n                    \"modelIdentifier\": evaluator_model\n                }]\n            }\n        }\n    }\n)\n\nprint(f\"Created model evaluation job: {model_eval_job_name}\")\nprint(f\"Job ID: {model_eval_job['jobArn']}\")</code></pre> \n  </div> </li> \n <li>After submitting the evaluation job, monitor its status with <code>get_evaluation_job</code> and access results at your specified Amazon S3 location when complete, including the standard and custom metric performance data.</li> \n</ol> \n<h2>Create a RAG system evaluation with custom metrics using Amazon Bedrock Evaluations</h2> \n<p>In this example, we walk through a RAG system evaluation with a combination of built-in metrics and custom evaluation metrics on the Amazon Bedrock console. Complete the following steps:</p> \n<ol> \n <li>On the Amazon Bedrock console, choose <strong>Evaluations</strong> in the navigation pane.</li> \n <li>On the <strong>RAG</strong> tab, choose <strong>Create</strong>.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105997\" height=\"590\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image017.jpg\" width=\"1288\" /></li> \n <li>For the <strong>RAG</strong> <strong>evaluation details</strong>, enter an evaluation name and optional description.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105998\" height=\"705\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image019.jpg\" width=\"1443\" /></li> \n <li>For <strong>Evaluator model</strong>, choose the model you want to use for automatic evaluation. The evaluator model selected here will be used to calculate default metrics if selected. For this example, we chose <strong>Claude 3.5 Sonnet</strong> as the evaluator model.</li> \n <li>Include any optional tags.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-105999\" height=\"568\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image021.jpg\" width=\"1435\" /></li> \n <li>For <strong>Inference source, </strong>select the source. Here, you have the option to select between <strong>Bedrock Knowledge Bases</strong> and <strong>Bring your own inference responses</strong>. If you’re using <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock Knowledge Bases</a>, you will need to choose a previously created knowledge base or create a new one. For BYOI responses, you can bring the prompt dataset, context, and output from a RAG system. For this example, we chose <strong>Bedrock Knowledge Base</strong> as our inference source.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106000\" height=\"590\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image023.jpg\" width=\"1425\" /></li> \n <li>Specify the evaluation type, response generator model, and built-in metrics. You can choose between a combined retrieval and response evaluation or a retrieval only evaluation, with options to use default metrics, custom metrics, or both for your RAG evaluation. The response generator model is only required when using an Amazon Bedrock knowledge base as the inference source. For the BYOI configuration, you can proceed without a response generator. For this example, we selected <strong>Retrieval and response generation</strong> as our evaluation type and chose <strong>Nova Lite 1.0</strong> as our response generator model.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106001\" height=\"1250\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image025.jpg\" width=\"1013\" /></li> \n <li>In the <strong>Custom Metrics</strong> section, choose your evaluator model. We selected <strong>Claude 3.5 Sonnet v1</strong> as our evaluator model for custom metrics.</li> \n <li>Choose <strong>Add custom metrics</strong>.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106002\" height=\"373\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image027.jpg\" width=\"1022\" /></li> \n <li>Create your new metric. For this example, we create a new custom metric for our RAG evaluation called <code>information_comprehensiveness</code>. This metric evaluates how thoroughly and completely the response addresses the query by using the retrieved information. It measures the extent to which the response extracts and incorporates relevant information from the retrieved passages to provide a comprehensive answer.</li> \n <li>You can choose between importing a JSON file, using a preconfigured template, or creating a custom metric with full configuration control. For example, you can select the preconfigured templates for the default metrics and change the scoring system or rubric. For our <code>information_comprehensiveness</code> metric, we select the custom option, which allows us to input our evaluator prompt directly.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106003\" height=\"681\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image029.jpg\" width=\"902\" /></li> \n <li>For <strong>Instructions</strong>, enter your prompt. For example: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\">Your role is to evaluate how comprehensively the response addresses the query \nusing the retrieved information. Assess whether the response provides a thorough \ntreatment of the subject by effectively utilizing the available retrieved passages.\n\nCarefully evaluate the comprehensiveness of the RAG response for the given query\n against all specified criteria. Assign a single overall score that best represents\n the comprehensiveness, and provide a brief explanation justifying your rating, \nreferencing specific strengths and weaknesses observed.\n\nWhen evaluating response comprehensiveness, consider the following rubrics:\n- Coverage: Does the response utilize the key relevant information from the retrieved\n passages?\n- Depth: Does the response provide sufficient detail on important aspects from the\n retrieved information?\n- Context utilization: How effectively does the response leverage the available\n retrieved passages?\n- Information synthesis: Does the response combine retrieved information to create\n a thorough treatment?\n\nEvaluate the following:\n\nQuery: {{prompt}}\n\nRetrieved passages: {{context}}\n\nResponse to evaluate: {{prediction}}</code></pre> \n  </div> </li> \n <li>Enter your output schema to define how the custom metric results will be structured, visualized, normalized (if applicable), and explained by the model.</li> \n</ol> \n<p>If you use the built-in output schema (recommended), do not add your rating scale into the main judge prompt. The evaluation service will automatically concatenate your judge prompt instructions with your defined output schema rating scale and some structured output instructions (unique to each judge model) behind the scenes so that your judge model results can be parsed. The fully concatenated judge prompts are visible in the <strong>Preview</strong> window if you are using the Amazon Bedrock console to construct your custom metrics.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-106004\" height=\"851\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image031.jpg\" width=\"917\" /></p> \n<ol start=\"14\"> \n <li>For <strong>Dataset and evaluation results S3 location</strong>, enter your input and output locations in Amazon S3.</li> \n <li>For <strong>Amazon Bedrock IAM role – Permissions</strong>, select <strong>Use an existing service role</strong> and choose your role.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106005\" height=\"955\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image033.jpg\" width=\"1115\" /></li> \n <li>Choose <strong>Create</strong> and wait for the job to complete.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-106006\" height=\"361\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/ML-18787-image035.jpg\" width=\"1287\" /></li> \n</ol> \n<h2>Start a RAG evaluation job with custom metrics using the Python SDK and APIs</h2> \n<p>To use the Python SDK for creating an RAG evaluation job with custom metrics, follow these steps (or refer to our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-observe/bedrock-eval-custom-metrics\" rel=\"noopener\" target=\"_blank\">example notebook</a>):</p> \n<ol> \n <li>Set up the required configurations, which should include your model identifier for the default metrics and custom metrics evaluator, IAM role with appropriate permissions, knowledge base ID, Amazon S3 paths for input data containing your inference responses, and output location for results: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\">import boto3\nimport time\nfrom datetime import datetime\n\n# Configure knowledge base and model settings\nknowledge_base_id = \"<YOUR_KB_ID>\"\nevaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\ngenerator_model = \"amazon.nova-lite-v1:0\"\ncustom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\nrole_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\nBUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n\n# Specify S3 locations\ninput_data = f\"s3://{BUCKET_NAME}/evaluation_data/input.jsonl\"\noutput_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n\n# Configure retrieval settings\nnum_results = 10\nsearch_type = \"HYBRID\"\n\n# Create Bedrock client\n# NOTE: You can change the region name to the region of your choosing\nbedrock_client = boto3.client('bedrock', region_name='us-east-1') </code></pre> \n  </div> </li> \n <li>To define a custom metric for RAG evaluation, create a JSON structure with a <code>customMetricDefinition</code> Include your metric’s name, write detailed evaluation instructions incorporating template variables (such as <code>{{prompt}}</code>, <code>{{context}}</code>, and <code>{{prediction}})</code>, and define your <code>ratingScale</code> array with assessment values using either numerical scores (<code>floatValue</code>) or categorical labels (<code>stringValue</code>). This properly formatted JSON schema enables Amazon Bedrock to evaluate responses consistently according to your specific criteria. \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\"># Define our custom information_comprehensiveness metric\ninformation_comprehensiveness_metric = {\n    \"customMetricDefinition\": {\n        \"name\": \"information_comprehensiveness\",\n        \"instructions\": \"\"\"\n        Your role is to evaluate how comprehensively the response addresses the \nquery using the retrieved information. \n        Assess whether the response provides a thorough treatment of the subject\nby effectively utilizing the available retrieved passages.\n\nCarefully evaluate the comprehensiveness of the RAG response for the given query\nagainst all specified criteria. \nAssign a single overall score that best represents the comprehensiveness, and \nprovide a brief explanation justifying your rating, referencing specific strengths\nand weaknesses observed.\n\nWhen evaluating response comprehensiveness, consider the following rubrics:\n- Coverage: Does the response utilize the key relevant information from the \nretrieved passages?\n- Depth: Does the response provide sufficient detail on important aspects from \nthe retrieved information?\n- Context utilization: How effectively does the response leverage the available \nretrieved passages?\n- Information synthesis: Does the response combine retrieved information to \ncreate a thorough treatment?\n\nEvaluate using the following:\n\nQuery: {{prompt}}\n\nRetrieved passages: {{context}}\n\nResponse to evaluate: {{prediction}}\n\"\"\",\n        \"ratingScale\": [\n            {\n                \"definition\": \"Very comprehensive\",\n                \"value\": {\n                    \"floatValue\": 3\n                }\n            },\n            {\n                \"definition\": \"Moderately comprehensive\",\n                \"value\": {\n                    \"floatValue\": 2\n                }\n            },\n            {\n                \"definition\": \"Minimally comprehensive\",\n                \"value\": {\n                    \"floatValue\": 1\n                }\n            },\n            {\n                \"definition\": \"Not at all comprehensive\",\n                \"value\": {\n                    \"floatValue\": 0\n                }\n            }\n        ]\n    }\n}</code></pre> \n  </div> </li> \n <li>To create a RAG evaluation job with custom metrics, use the <code>create_evaluation_job</code> API and include your custom metric in the <code>customMetricConfig</code> section, specifying both built-in metrics (<code>Builtin.Correctness</code>) and your custom metric in the <code>metricNames</code> array. Configure the job with your knowledge base ID, generator model, evaluator model, and proper Amazon S3 paths for input dataset and output results. \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\"># Create the evaluation job\nretrieve_generate_job_name = f\"rag-evaluation-generate-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\nretrieve_generate_job = bedrock_client.create_evaluation_job(\n    jobName=retrieve_generate_job_name,\n    jobDescription=\"Evaluate retrieval and generation with custom metric\",\n    roleArn=role_arn,\n    applicationType=\"RagEvaluation\",\n    inferenceConfig={\n        \"ragConfigs\": [{\n            \"knowledgeBaseConfig\": {\n                \"retrieveAndGenerateConfig\": {\n                    \"type\": \"KNOWLEDGE_BASE\",\n                    \"knowledgeBaseConfiguration\": {\n                        \"knowledgeBaseId\": knowledge_base_id,\n                        \"modelArn\": generator_model,\n                        \"retrievalConfiguration\": {\n                            \"vectorSearchConfiguration\": {\n                                \"numberOfResults\": num_results\n                            }\n                        }\n                    }\n                }\n            }\n        }]\n    },\n    outputDataConfig={\n        \"s3Uri\": output_path\n    },\n    evaluationConfig={\n        \"automated\": {\n            \"datasetMetricConfigs\": [{\n                \"taskType\": \"General\",\n                \"dataset\": {\n                    \"name\": \"RagDataset\",\n                    \"datasetLocation\": {\n                        \"s3Uri\": input_data\n                    }\n                },\n                \"metricNames\": [\n                    \"Builtin.Correctness\",\n                    \"Builtin.Completeness\",\n                    \"Builtin.Helpfulness\",\n                    \"information_comprehensiveness\"\n                ]\n            }],\n            \"evaluatorModelConfig\": {\n                \"bedrockEvaluatorModels\": [{\n                    \"modelIdentifier\": evaluator_model\n                }]\n            },\n            \"customMetricConfig\": {\n                \"customMetrics\": [\n                    information_comprehensiveness_metric\n                ],\n                \"evaluatorModelConfig\": {\n                    \"bedrockEvaluatorModels\": [{\n                        \"modelIdentifier\": custom_metrics_evaluator_model\n                    }]\n                }\n            }\n        }\n    }\n)\n\nprint(f\"Created evaluation job: {retrieve_generate_job_name}\")\nprint(f\"Job ID: {retrieve_generate_job['jobArn']}\")</code></pre> \n  </div> </li> \n <li>After submitting the evaluation job, you can check its status using the <code>get_evaluation_job</code> method and retrieve the results when the job is complete. The output will be stored at the Amazon S3 location specified in the <code>output_path</code> parameter, containing detailed metrics on how your RAG system performed across the evaluation dimensions including custom metrics.</li> \n</ol> \n<p>Custom metrics are only available for LLM-as-a-judge. At the time of writing, we don’t accept custom <a href=\"http://aws.amazon.com/lambda\" rel=\"noopener\" target=\"_blank\">AWS Lambda</a> functions or endpoints for code-based custom metric evaluators. Human-based model evaluation has supported <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-jobs-management-create-human.html\" rel=\"noopener\" target=\"_blank\">custom metric definition</a> since its launch in November 2023.</p> \n<h2>Clean up</h2> \n<p>To avoid incurring future charges, delete the S3 bucket, notebook instances, and other resources that were deployed as part of the post.</p> \n<h2>Conclusion</h2> \n<p>The addition of custom metrics to Amazon Bedrock Evaluations empowers organizations to define their own evaluation criteria for generative AI systems. By extending the LLM-as-a-judge framework with custom metrics, businesses can now measure what matters for their specific use cases alongside built-in metrics. With support for both numerical and categorical scoring systems, these custom metrics enable consistent assessment aligned with organizational standards and goals.</p> \n<p>As generative AI becomes increasingly integrated into business processes, the ability to evaluate outputs against custom-defined criteria is essential for maintaining quality and driving continuous improvement. We encourage you to explore these new capabilities through the Amazon Bedrock console and API examples provided, and discover how <a href=\"https://aws.amazon.com/bedrock/evaluations/\" rel=\"noopener\" target=\"_blank\">personalized evaluation frameworks</a> can enhance your AI systems’ performance and business impact.</p> \n<hr /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-thumbnail wp-image-93420 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/20/image7-1-100x100.jpeg\" width=\"100\" /><strong>Shreyas Subramanian</strong> is a Principal Data Scientist and helps customers by using generative AI and deep learning to solve their business challenges using AWS services. Shreyas has a background in large-scale optimization and ML and in the use of ML and reinforcement learning for accelerating optimization tasks.</p> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/wale_picture_blog.png\" rel=\"noopener\" target=\"_blank\"><img alt=\"\" class=\"size-full wp-image-101077 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/wale_picture_blog.png\" width=\"100\" /></a><strong>Adewale Akinfaderin</strong> is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.</p> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/Badgephoto-1.jpeg\" rel=\"noopener\" target=\"_blank\"><img alt=\"\" class=\"size-full wp-image-101078 alignleft\" height=\"115\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/Badgephoto-1.jpeg\" width=\"100\" /></a><strong>Jesse Manders</strong> is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.</p> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/blog-image-ishansin-1.jpeg\" rel=\"noopener\" target=\"_blank\"><img alt=\"\" class=\"size-full wp-image-101075 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/blog-image-ishansin-1.jpeg\" width=\"100\" /></a><strong>Ishan Singh</strong> is a Sr. Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p>",
    "source_feed": "https://aws.amazon.com/blogs/machine-learning/feed/",
    "scraped_at_iso": "2025-05-06T21:46:53Z",
    "selected_image_url": "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/featured-images-ML-18787-1120x630.jpg",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "AI Models",
        "reasoning_summary": "The article discusses Amazon Bedrock's new tools for evaluating foundation models and RAG systems, which is a significant development in AI model evaluation and involves a key company (Amazon).",
        "primary_topic_keyword": "AI model evaluation"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-06T21:47:01Z",
    "topic": "AI Models",
    "is_breaking": false,
    "primary_keyword": "AI model evaluation",
    "seo_agent_results": {
        "generated_title_tag": "AI Model Evaluation with Custom Metrics in Amazon Bedrock",
        "generated_meta_description": "Learn how Amazon Bedrock's custom metrics enhance AI model evaluation for generative AI applications, ensuring tailored performance assessment.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"AI Model Evaluation with Custom Metrics in Amazon Bedrock\",  \n  \"description\": \"Learn how Amazon Bedrock's custom metrics enhance AI model evaluation for generative AI applications, ensuring tailored performance assessment.\",  \n  \"keywords\": [\"AI model evaluation\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/06/featured-images-ML-18787-1120x630.jpg\" },  \n  \"datePublished\": \"2025-05-06T21:39:21Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://dacoolaa.netlify.app\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Use Custom Metrics to Evaluate Your Generative AI Application with Amazon Bedrock  \n\nAmazon Bedrock now supports custom metrics for evaluating foundation models (FMs) and Retrieval Augmented Generation (RAG) systems, whether hosted on Bedrock or third-party platforms. The LLM-as-a-judge technique automates assessments for quality and responsible AI, including correctness, completeness, and harmfulness. Businesses can now define bespoke metrics—like brand voice adherence or domain-specific scoring—to align evaluations with unique requirements.  \n\n### Why It Matters  \nCustom metrics empower organizations to measure AI performance beyond standard benchmarks, ensuring generative AI outputs meet specific business needs. This flexibility is crucial for industries requiring tailored evaluations, such as compliance-heavy sectors or brand-sensitive applications. By integrating custom scoring, Amazon Bedrock enhances transparency and control, enabling continuous improvement in AI-driven solutions."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "Amazon Bedrock",
        "Generative AI",
        "Custom Metrics",
        "Foundation Models",
        "Retrieval Augmented Generation",
        "LLM-as-a-judge",
        "Responsible AI",
        "AI Evaluation",
        "Business AI Solutions",
        "AI Performance Metrics"
    ],
    "tags_agent_error": null,
    "trend_score": 15.0,
    "slug": "use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedroc",
    "audio_url": null
}