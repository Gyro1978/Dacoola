{
    "id": "d477e8bcad7566ac0dc90523b6a36e43d1f5541ce5b508f5b2b07daa369d4ee4",
    "title": "Amazon Bedrock Model Distillation: Boost function calling accuracy while reducing cost and latency",
    "link": "https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-model-distillation-boost-function-calling-accuracy-while-reducing-cost-and-latency/",
    "published_iso": "2025-05-01T01:04:26Z",
    "summary": "<p><a href=\"https://aws.amazon.com/bedrock/model-distillation/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock Model Distillation</a> is generally available, and it addresses the fundamental challenge many organizations face when deploying <a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener\" target=\"_blank\">generative AI</a>: how to maintain high performance while reducing costs and latency. This technique transfers knowledge from larger, more capable <a href=\"https://aws.amazon.com/what-is/foundation-models/\" rel=\"noopener\" target=\"_blank\">foundation models</a> (FMs) that act as teachers to smaller, more efficient models (students), creating specialized models that excel at specific tasks. In this post, we highlight the advanced data augmentation techniques and performance improvements in Amazon Bedrock Model Distillation with Meta’s Llama model family.</p> \n<p>Agent function calling represents a critical capability for modern AI applications, allowing models to interact with external tools, databases, and APIs by accurately determining when and how to invoke specific functions. Although larger models typically excel at identifying the appropriate functions to call and constructing proper parameters, they come with higher costs and latency. Amazon Bedrock Model Distillation now enables smaller models to achieve comparable function calling accuracy while delivering substantially faster response times and lower operational costs.</p> \n<p>The value proposition is compelling: organizations can deploy AI agents that maintain high accuracy in tool selection and parameter construction while benefiting from the reduced footprint and increased throughput of smaller models. This advancement makes sophisticated agent architectures more accessible and economically viable across a broader range of applications and scales of deployment.</p> \n<h2>Prerequisites</h2> \n<p>For a successful implementation of Amazon Bedrock Model Distillation, you’ll need to meet several requirements. We recommend referring to the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/submit-model-distillation-job.html\" rel=\"noopener\" target=\"_blank\">Submit a model distillation job in Amazon Bedrock</a> in the official AWS documentation for the most up-to-date and comprehensive information.</p> \n<p>Key requirements include:</p> \n<ul> \n <li>An active <a href=\"https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&client_id=signup\" rel=\"noopener\" target=\"_blank\">AWS account</a></li> \n <li>Selected teacher and student models enabled in your account (verify on the Model access page of the Amazon Bedrock console)</li> \n <li>An <a href=\"https://aws.amazon.com/s3\" rel=\"noopener\" target=\"_blank\">S3 bucket</a> for storing input datasets and output artifacts</li> \n <li>Appropriate <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener\" target=\"_blank\">IAM permissions</a>:</li> \n <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html#model-customization-iam-role-trust\" rel=\"noopener\" target=\"_blank\">Trust relationship</a> allowing Amazon Bedrock to assume the role</li> \n <li>Permissions to access S3 for input/output data and invocation logs</li> \n <li>Permissions for model inference when using inference profiles</li> \n</ul> \n<p>If you’re using historical invocation logs, confirm if model invocation logging is enabled in your Amazon Bedrock settings with S3 selected as the logging destination.</p> \n<h2>Preparing your data</h2> \n<p>Effective data preparation is crucial for successful distillation of agent function calling capabilities. <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock</a> provides two primary methods for preparing your training data: uploading JSONL files to Amazon S3 or using historical invocation logs. Regardless of which method to choose, you’ll need to prepare proper formatting of tool specifications to enable successful agent function calling distillation.</p> \n<h3>Tool specification format requirements</h3> \n<p>For agent function calling distillation, Amazon Bedrock requires that tool specifications be provided as part of your training data. These specifications must be encoded as text within the system or user message of your input data. The example shown is using the Llama model family’s function calling format:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-code\">system: 'You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n\nHere is a list of functions in JSON format that you can invoke.\n[\n    {\n        \"name\": \"lookup_weather\",\n        \"description\": \"Lookup weather to a specific location\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\n                \"city\"\n            ],\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                },\n                \"date\": {\n                    \"type\": \"string\",\n                }\n            }\n        }\n    }\n ]'\n user: \"What's the weather tomorrow?\"</code></pre> \n </div> \n</div> \n<p>This approach lets the model learn how to interpret tool definitions and make appropriate function calls based on user queries. Afterwards, when calling inference on the distilled student model, we suggest keeping the prompt format consistent with the distillation input data. This provides optimal performance by maintaining the same structure the model was trained on.</p> \n<h3>Preparing data using Amazon S3 JSONL upload</h3> \n<p>When creating a JSONL file for distillation, each record must follow this structure:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{\n    \"schemaVersion\": \"bedrock-conversation-2024\",\n    \"system\": [\n        {\n            \"text\": 'You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n                    Here is a list of functions in JSON format that you can invoke.\n                    [\n                        {\n                            \"name\": \"lookup_weather\",\n                            \"description\": \"Lookup weather to a specific location\",\n                            \"parameters\": {\n                                \"type\": \"dict\",\n                                \"required\": [\n                                    \"city\"\n                                ],\n                                \"properties\": {\n                                    \"location\": {\n                                        \"type\": \"string\",\n                                    },\n                                    \"date\": {\n                                        \"type\": \"string\",\n                                    }\n                                }\n                            }\n                        }\n                    ]'\n        }\n    ],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"text\": \"What's the weather tomorrow?\"\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n               {\n                   \"text\": \"[lookup_weather(location=\\\"san francisco\\\", date=\\\"tomorrow\\\")]\"\n               }\n            ]\n        }\n    ]\n}</code></pre> \n</div> \n<p>Each record must include the <code>schemaVersion</code> field with the value <code>bedrock-conversation-2024</code>. The system field contains instructions for the model, including available tools. The messages field contains the conversation, with <strong>required</strong> user input and <strong>optional</strong> assistant responses.</p> \n<h3>Using historical invocation logs</h3> \n<p>Alternatively, you can use your historical model invocation logs on Amazon Bedrock for distillation. This approach uses actual production data from your application, capturing real-world function calling scenarios. To use this method:</p> \n<ol> \n <li>Enable invocation logging in your Amazon Bedrock account settings, selecting S3 as your logging destination.</li> \n <li>Add metadata to your model invocations using the <code>requestMetadata</code> field to categorize interactions. For example: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\">\"requestMetadata\": { \n   \"project\": \"WeatherAgent\", \n   \"intent\": \"LocationQuery\", \n   \"priority\": \"High\"\n}</code></pre> \n  </div> </li> \n <li>When creating your distillation job, specify filters to select relevant logs based on metadata: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\">\"requestMetadataFilters\": { \n    \"equals\": {\"project\": \"WeatherAgent\"} \n}</code></pre> \n  </div> </li> \n</ol> \n<p>Using historical invocation logs means that you can distill knowledge from your production workloads, allowing the model to learn from real user interactions and function calls.</p> \n<h2>Model distillation enhancements</h2> \n<p>Although the basic process for creating a model distillation job remains similar to what we described in our <a href=\"https://aws.amazon.com/blogs/machine-learning/a-guide-to-amazon-bedrock-model-distillation-preview/\" rel=\"noopener\" target=\"_blank\">previous blog post</a>, Amazon Bedrock Model Distillation introduces several enhancements with general availability that improve the experience, capabilities, and transparency of the service.</p> \n<h3>Expanded model support</h3> \n<p>With general availability, we have expanded the model options available for distillation. In addition to the models supported during preview, customers can now use:</p> \n<ul> \n <li>Nova Premier as a teacher model for Nova Pro/Lite/Micro models distillation</li> \n <li>Anthropic Claude Sonnet 3.5 v2 as a teacher model for Claude Haiku distillation</li> \n <li>Meta’s Llama 3.3 70B as teacher and 3.2 1B and 3B as student models for Meta model distillation</li> \n</ul> \n<p>This broader selection allows customers to find the balance between performance and efficiency across different use cases. For the most current list of supported models, refer to the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-distillation-supported.html\" rel=\"noopener\" target=\"_blank\">Amazon Bedrock documentation</a>.</p> \n<h3>Advanced data synthesis technology</h3> \n<p>Amazon Bedrock applies proprietary data synthesis techniques during the distillation process for certain use cases. This science innovation automatically generates additional training examples that improve the student model’s ability to generate better response.</p> \n<p>For agent function calling with Llama models specifically, the data augmentation methods help bridge the performance gap between teacher and student models compared to vanilla distillation (<em>vanilla distillation</em> means directly annotating input data with teacher response and run student training with supervised fine-tuning). This makes the student models’ performance much more comparable to the teacher after distillation while maintaining the cost and latency benefits of a smaller model.</p> \n<h3>Enhanced training visibility</h3> \n<p>Amazon Bedrock model distillation now provides better visibility into the training process through multiple enhancements:</p> \n<ol> \n <li><strong>Synthetic data transparency</strong> – Model distillation now provides samples of the synthetically generated training data used to enhance model performance. For most model families, up to 50 sample prompts are exported (up to 25 for Anthropic models), giving you insight into how your model was trained, which can help support internal compliance requirements.</li> \n <li><strong>Prompt insights reporting</strong> – A summarized report of prompts accepted for distillation is provided, along with detailed visibility into prompts that were rejected and the specific reason for rejection. This feedback mechanism helps you identify and fix problematic prompts to improve your distillation success rate.</li> \n</ol> \n<p>These insights are stored in the output S3 bucket specified during job creation, giving you a clearer picture of the knowledge transfer process.</p> \n<h3>Improved job status reporting</h3> \n<p>Amazon Bedrock Model Distillation also offers enhanced training job status reporting to provide more detailed information about where your model distillation job stands in the process. Rather than brief status indicators such as “In Progress” or “Complete,” the system now provides more granular status updates, helping you better track the progress of the distillation job.</p> \n<p>You can track these job status details in both the <a href=\"https://aws.amazon.com/console/\">AWS Management Console</a> and <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDK</a>.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105555\" height=\"482\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/ML-18742-image-1.png\" width=\"2404\" /></p> \n<h2>Performance improvements and benefits</h2> \n<p>Now that we’ve explored the feature enhancements in Amazon Bedrock Model Distillation, we examine the benefits these capabilities deliver, particularly for agent function calling use cases.</p> \n<h3>Evaluation metric</h3> \n<p>We use abstract syntax tree (AST) to evaluate the function calling performance. AST parses the generated function call and performs fine-grained evaluation on the correctness of the generated function name, parameter values, and data types with the following workflow:</p> \n<ol> \n <li><strong>Function matching</strong> – Checks if the predicted function name is consistent with one of the possible answers</li> \n <li><strong>Required parameter matching</strong> – Extracts the arguments from the AST and checks if each parameter can be found and exact matched in possible answers</li> \n <li><strong>Parameter type and value matching</strong> – Checks if the predicted parameter values and types are correct</li> \n</ol> \n<p>The process is illustrated in following diagram from <a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\" rel=\"noopener\" target=\"_blank\">Gorilla: Large Language Model Connected with Massive APIs</a>.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105556\" height=\"739\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/ML-18742-image-2.png\" width=\"1429\" /></p> \n<h3>Experiment results</h3> \n<p>To evaluate model distillation in the function call use case, we used the <a href=\"https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html\" rel=\"noopener\" target=\"_blank\">BFCL v2 dataset</a> and filtered it to specific domains (entertainment, in this case) to match a typical use case of model customization. We also split the data into training and test sets and performed distillation on the training data while we ran evaluations on the test set. Both the training set and the test set contained around 200 examples. We assessed the performance of several models, including the teacher model (Llama 405B), the base student model (Llama 3B), a vanilla distillation version where Llama 405B is distilled into Llama 3B without data augmentation, and an advanced distillation version enhanced with proprietary data augmentation techniques.</p> \n<p>The evaluation focused on simple and multiple categories defined in the BFCL V2 dataset. As shown in the following chart, there is a performance variance between the teacher and the base student model across both categories. Vanilla distillation significantly improved the base student model’s performance. In the simple category, performance increased from 0.478 to 0.783, representing a 63.8% relative improvement. In the multiple category, the score rose from 0.586 to 0.742, which is a 26.6% relative improvement. On average, vanilla distillation led to a 45.2% improvement across the two categories.</p> \n<p>Applying data augmentation techniques provided further gains beyond vanilla distillation. In the simple category, performance improved from 0.783 to 0.826, and in the multiple category, from 0.742 to 0.828. On average, this resulted in a 5.8% relative improvement across both categories, calculated as the mean of the relative gains in each. These results highlight the effectiveness of both distillation and augmentation strategies in enhancing student model performance for function call tasks.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105557\" height=\"972\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/ML-18742-image-3.png\" width=\"1992\" /></p> \n<p>We show the latency and output speed comparison for different models in the following figure. The data is gathered from <a href=\"https://artificialanalysis.ai/\" rel=\"noopener\" target=\"_blank\">Artificial Analysis</a>, a website that provides independent analysis of AI models and providers, on April 4, 2025. We find that there is a clear trend on latency and generation speed as different size Llama models evaluated. Notably, the Llama 3.1 8B model offers the highest output speed, making it the most efficient in terms of responsiveness and throughput. Similarly, Llama 3.2 3B performs well with a slightly higher latency but still maintains a solid output speed. On the other hand, Llama 3.1 70B and Llama 3.1 405B exhibit much higher latencies with significantly lower output speeds, indicating a substantial performance cost at higher model sizes. Compared to Llama 3.1 405B, Llama 3.2 3B provides 72% latency reduction and 140% output speed improvement. These results suggest that smaller models might be more suitable for applications where speed and responsiveness are critical.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105558\" height=\"590\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/ML-18742-image-4.png\" width=\"790\" /></p> \n<p>In addition, we report the comparison of cost per 1M tokens for different Llama models. As shown in the following figure, it’s evident that smaller models (Llama 3.2 3B and Llama 3.1 8B) are significantly more cost-effective. As the model size increases (Llama 3.1 70B and Llama 3.1 405B), the pricing scales steeply. This dramatic increase underscores the trade-off between model complexity and operational cost.</p> \n<p>Real-world agent applications require LLM models that can achieve a good balance between accuracy, speed, and cost. This result shows that using a distilled model for agent applications helps developers receive the speed and cost of smaller models while getting similar accuracy as a larger teacher model.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-105559\" height=\"590\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/ML-18742-image-5.png\" width=\"790\" /></p> \n<h2>Conclusion</h2> \n<p>Amazon Bedrock Model Distillation is now generally available, offering organizations a practical pathway for deploying capable agent experiences without compromising on performance or cost-efficiency. As our performance evaluation demonstrates, distilled models for function calling can achieve accuracy comparable to models many times their size while delivering significantly faster inference and lower operational costs. This capability enables scalable deployment of AI agents that can accurately interact with external tools and systems across enterprise applications.</p> \n<p>Start using Amazon Bedrock Model Distillation today through the AWS Management Console or API to transform your generative AI applications, including agentic use cases, with the balance of accuracy, speed, and cost efficiency. For implementation examples, check out our code samples in the <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/custom-models/model_distillation\" rel=\"noopener\" target=\"_blank\">amazon-bedrock-samples</a> GitHub repository.</p> \n<h2>Appendix</h2> \n<p><strong>BFCL V2 simple category</strong></p> \n<p><strong>Definition: </strong>The simple category consists of tasks where the user is provided with a single function documentation (that is, one JSON function definition), and the model is expected to generate exactly one function call that matches the user’s request. This is the most basic and commonly encountered scenario, focusing on whether the model can correctly interpret a straightforward user query and map it to the only available function, filling in the required parameters as needed.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\"># Example\n{\n    \"id\": \"live_simple_0-0-0\",\n    \"question\": [\n        [{\n            \"role\": \"user\",\n            \"content\": \"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\"\n        }]\n    ],\n    \"function\": [{\n        \"name\": \"get_user_info\",\n        \"description\": \"Retrieve details for a specific user by their unique identifier.\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\"user_id\"],\n            \"properties\": {\n                \"user_id\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n                },\n                \"special\": {\n                    \"type\": \"string\",\n                    \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n                    \"default\": \"none\"\n                }\n            }\n        }\n    }]\n}</code></pre> \n</div> \n<p><strong>BFCL V2 multiple category</strong></p> \n<p><strong>Definition: </strong>The multiple category presents the model with a user query and several (typically two to four) function documentations. The model must select the most appropriate function to call based on the user’s intent and context and then generate a single function call accordingly. This category evaluates the model’s ability to understand the user’s intent, distinguish between similar functions, and choose the best match from multiple options.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">{\n    \"id\": \"live_multiple_3-2-0\",\n    \"question\": [\n        [{\n            \"role\": \"user\",\n            \"content\": \"Get weather of Ha Noi for me\"\n        }]\n    ],\n    \"function\": [{\n        \"name\": \"uber.ride\",\n        \"description\": \"Finds a suitable Uber ride for the customer based on the starting location, the desired ride type, and the maximum wait time the customer is willing to accept.\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\"loc\", \"type\", \"time\"],\n            \"properties\": {\n                \"loc\": {\n                    \"type\": \"string\",\n                    \"description\": \"The starting location for the Uber ride, in the format of 'Street Address, City, State', such as '123 Main St, Springfield, IL'.\"\n                },\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of Uber ride the user is ordering.\",\n                    \"enum\": [\"plus\", \"comfort\", \"black\"]\n                },\n                \"time\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The maximum amount of time the customer is willing to wait for the ride, in minutes.\"\n                }\n            }\n        }\n    }, {\n        \"name\": \"api.weather\",\n        \"description\": \"Retrieve current weather information for a specified location.\",\n        \"parameters\": {\n            \"type\": \"dict\",\n            \"required\": [\"loc\"],\n            \"properties\": {\n                \"loc\": {\n                    \"type\": \"string\",\n                    \"description\": \"The location for which weather information is to be retrieved, in the format of 'City, Country' (e.g., 'Paris, France').\"\n                }\n            }\n        }\n    }]\n}</code></pre> \n</div> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-78760 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/06/16/yanyan.png\" width=\"100\" />Yanyan Zhang</strong> is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-101075\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/06/blog-image-ishansin-1.jpeg\" width=\"100\" />Ishan Singh</strong> is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-105597\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/yijunt-100x133.jpg\" width=\"100\" /> Yijun Tian</strong> is an Applied Scientist II at AWS Agentic AI, where he focuses on advancing fundamental research and applications in Large Language Models, Agents, and Generative AI. Prior to joining AWS, he obtained his Ph.D. in Computer Science from the University of Notre Dame.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-105596\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/yawenwan-100x133.jpg\" width=\"100\" />Yawei Wang</strong> is an Applied Scientist at AWS Agentic AI, working at the forefront of generative AI technologies to build next-generation AI products within AWS. He also collaborates with AWS business partners to identify and develop machine learning solutions that address real-world industry challenges.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-105599\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/qiaojiny-100x133.jpg\" width=\"100\" />David Yan</strong> is a Senior Research Engineer at AWS Agentic AI, leading efforts in Agent Customization and Optimization. Prior to that, he was in AWS Bedrock, leading model distillation effort to help customers optimize LLM latency, cost and accuracy. His research interest includes AI agent, planning and prediction and inference optimization. Before joining AWS, David worked on planning and behavior prediction for autonomous driving in Waymo. Before that, he worked on nature language understanding for knowledge graph at Google. David received a M.S. in Electrical Engineering from Stanford University and a B.S. in Physics from Peking University.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-105598\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/29/xupanpan-100x133.jpg\" width=\"100\" />Panpan Xu</strong> is a Principal Applied Scientist at AWS Agentic AI, leading a team working on Agent Customization and Optimization. Prior to that, she lead a team in AWS Bedrock working on research and development of inference optimization techniques for foundation models, covering modeling level techniques such as model distillation and sparsification to hardware-aware optimization. Her past research interest covers a broad range of topics including model interpretability, graph neural network, human-in-the-loop AI and interactive data visualization. Prior to joining AWS, she was a lead research scientist at Bosch Research and obtained her PhD in computer science from Hong Kong University of Science and Technology.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-95047\" height=\"99\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/12/02/shreeya.png\" width=\"100\" />Shreeya Sharma</strong> is a Senior Technical Product Manager at AWS, where she has been working on leveraging the power of generative AI to deliver innovative and customer-centric products. Shreeya holds a master’s degree from Duke University. Outside of work, she loves traveling, dancing, and singing.</p>",
    "source_feed": "https://aws.amazon.com/blogs/machine-learning/feed/",
    "scraped_at_iso": "2025-05-04T05:39:31Z",
    "selected_image_url": "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/30/abmd-1120x630.png",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "AI Models",
        "reasoning_summary": "The article reports on the general availability of Amazon Bedrock Model Distillation, a technique for improving function calling accuracy while reducing costs and latency, with specific mention of performance improvements using Meta's Llama model family. This presents new, verifiable information relevant to AI model deployment and optimization.",
        "primary_topic_keyword": "AI model distillation"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-04T05:44:28Z",
    "topic": "AI Models",
    "is_breaking": false,
    "primary_keyword": "AI model distillation",
    "seo_agent_results": {
        "generated_title_tag": "AI Model Distillation Boosts Accuracy, Cuts Costs in Amazon Bedrock",
        "generated_meta_description": "Amazon Bedrock's AI model distillation enhances function calling accuracy while reducing latency and costs, making generative AI more efficient.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"AI Model Distillation Boosts Accuracy, Cuts Costs in Amazon Bedrock\",  \n  \"description\": \"Amazon Bedrock's AI model distillation enhances function calling accuracy while reducing latency and costs, making generative AI more efficient.\",  \n  \"keywords\": [\"AI model distillation\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-model-distillation-boost-function-calling-accuracy-while-reducing-cost-and-latency/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/30/abmd-1120x630.png\" },  \n  \"datePublished\": \"2025-05-01T01:04:26Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://dacoolaa.netlify.app\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Amazon Bedrock Model Distillation: Boost function calling accuracy while reducing cost and latency  \n\nAmazon Bedrock Model Distillation is now generally available, offering a solution to a key challenge in deploying generative AI: balancing performance with cost and latency. The technique transfers knowledge from larger \"teacher\" foundation models (FMs) to smaller \"student\" models, optimizing them for specific tasks like function calling. This allows smaller models to match the accuracy of larger ones while delivering faster responses and lower operational costs. The update includes advanced data augmentation and performance improvements, particularly with Meta’s Llama model family.  \n\n### Why It Matters  \n\nAI model distillation addresses a critical bottleneck in AI adoption—high costs and slow response times from large models. By enabling smaller models to perform complex tasks like function calling with comparable accuracy, businesses can deploy AI agents more efficiently across various applications. This advancement lowers barriers to entry for enterprises looking to integrate generative AI into workflows, making sophisticated AI architectures more accessible and scalable. The focus on function calling—a core capability for AI agents interacting with APIs and tools—highlights its practical impact on real-world AI deployments."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "Amazon Bedrock",
        "Model Distillation",
        "Generative AI",
        "Function Calling",
        "AI Cost Optimization",
        "AI Latency Reduction",
        "Meta Llama Models",
        "AI Deployment",
        "Foundation Models",
        "AI Performance Improvement"
    ],
    "tags_agent_error": null,
    "trend_score": 14.47,
    "slug": "amazon-bedrock-model-distillation-boost-function-calling-accuracy-while-reducing",
    "audio_url": null,
    "post_template_hash": "7a34e3dba5945832e6f4e2288e1bdc5a445c645dd5f7ce6e39751b31c2a668db"
}