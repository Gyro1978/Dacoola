{
    "id": "b4824683ada96d7d9569647d5eb96d9932606709c2305775a481957a05d92689",
    "title": "Introducing MASS – A pre-training method that outperforms BERT and GPT in sequence to sequence language generation tasks",
    "link": "https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/",
    "published_iso": "2019-06-24T01:00:22Z",
    "summary": "<p><em>Editor’s note: Since 2018, pre-training has without a doubt become one of the hottest research topics in Natural Language Processing (NLP). By leveraging generalized language models like the BERT, GPT and XLNet, great breakthroughs have been achieved in natural language understanding. However, in sequence to sequence based language generation tasks, the popular pre-training methods have not achieved significant improvements. Now, researchers from Microsoft Research Asia have introduced MASS—a new pre-training method that achieves better results than BERT and GPT.</em></p>\n<p>BERT and XLNet have achieved great success in natural language understanding tasks (for example, sentiment classification, natural language inference, and SQuAD machine reading comprehension). However, besides natural language understanding tasks in NLP, there are other sequence to sequence based language generation tasks, such as neural machine translation, abstract summarization, conversational response generation, question answering, and text style transfer. For these tasks, encoder-attention-decoder is the dominant approach.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593671\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-1.png\"><img alt=\"Figure 1: The Encoder-Attention-Decoder framework.\" class=\"wp-image-593671 size-large\" height=\"199\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-1-1024x199.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593671\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 1: The Encoder-Attention-Decoder framework.</p></div></p>\n<p>As shown in Figure 1, the encoder takes the source sequence X as input and transforms it into a sequence of hidden representations, and then the decoder extracts the hidden representations from the encoder through an attention mechanism and generates a target sequence Y autoregressively.</p>\n<p>BERT and XLnet pre-train an encoder for natural language understanding, while GPT pre-trains a decoder for language modeling. We usually have to pre-train the encoder and decoder separately when leveraging BERT and GPT for sequence to sequence based language generation tasks. In such circumstances, the encoder-attention-decoder framework and the attention mechanism cannot be jointly trained. However, the attention mechanism is extremely important in these kinds of tasks and hinders BERT and GPT in achieving optimal performance.</p>\n<h3>A new pre-training method</h3>\n<p>With sequence to sequence based language generation tasks in mind, the Machine Learning Group at Microsoft Research Asia envisioned a new pre-training method. We called it MASS: Masked Sequence to Sequence Pre-training. MASS randomly masks a sentence fragment with length k and predicts this masked fragment through an encoder-attention-decoder framework.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593674\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-2.png\"><img alt=\"Figure 2: MASS framework.\" class=\"wp-image-593674 size-large\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-2-1024x200.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593674\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 2: MASS framework.</p></div></p>\n<p>As shown in the Figure 2, the 3rd-6th tokens of the sentence on the encoder side are masked, while on the decoder side, only the masked tokens are predicted and the other tokens are masked.</p>\n<p>MASS pre-training brings the following advantages:</p>\n<ul>\n<li>The other tokens on the decoder side (the tokens that are not masked on the encoder side) are masked, which can encourage the decoder to extract more information to help with the prediction of the sentence fragment. As a result, the encoder-attention-decoder is forced to be jointly pre-trained;</li>\n<li>In order to provide more useful information for the decoder, the encoder is forced to extract the meaning of the unmasked tokens on the encoder side, which can improve the capability of the encoder in language understanding;</li>\n<li>The decoder is designed to predict consecutive tokens (sentence fragments), which can improve the language modeling capability of the decoder.</li>\n</ul>\n<h3>General pre-training framework</h3>\n<p>MASS possesses an important hyperparameter k (the length of the masked fragment). By adjusting k, MASS can incorporate the masked language modeling in BERT and the standard language modeling in GPT, which extends MASS into a general pre-training framework.</p>\n<p>When k=1, according to the design of MASS, one token on the encoder side is masked, and the decoder side predicts this masked token, as shown in Figure 3. The decoder side has no input information and MASS is equivalent to the masked language model in BERT.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593677\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-3.png\"><img alt=\"Figure 3: k=1. One token on encoder side is masked; the decoder side predicts the masked token.\" class=\"wp-image-593677 size-large\" height=\"198\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-3-1024x198.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593677\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 3: k=1. One token on encoder side is masked; the decoder side predicts the masked token.</p></div></p>\n<p>When k=m (m is the length of the sequence), in MASS all tokens on the encoder side are masked, and the decoder side predicts all tokens, as shown in Figure 4. The decoder side cannot extract any information from the encoder side, and MASS is equivalent to the standard language model in GPT.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593680\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-4.png\"><img alt=\"Figure 4: k=m. All tokens on encoder side are masked; the decoder side predicts all tokens, just as in GPT.\" class=\"wp-image-593680 size-large\" height=\"196\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-4-1024x196.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593680\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 4: k=m. All tokens on encoder side are masked; the decoder side predicts all tokens, just as in GPT.</p></div></p>\n<p>The probability formulations of MASS under different values of k are shown in Table 1, where m is the length of the sequence, u and v are the start and end positions of the masked fragment respectively, Χ<sup>u:v</sup> represents the fragment from position u to v, and X<sup>\\u:v</sup> represents the sequence where the tokens from position u to v are masked. It can be seen that when k=1 or m, the probability formulation of MASS is equivalent to the masked language model in BERT and the standard language model in GPT.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593689\" style=\"width: 710px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Table-1.png\"><img alt=\"Table 1: Probability formulations of MASS under different values of k.\" class=\"wp-image-593689\" height=\"182\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Table-1-1024x266.png\" width=\"700\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593689\"><span class=\"sr-only\"> (opens in new tab)</span></a> Table 1: Probability formulations of MASS under different values of k.</p></div></p>\n<p>We conducted experiments to analyze the performance of MASS with different values of k, as shown in Figure 5.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593701\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-5-R.png\"><img alt=\"Figure 5: MASS performance under various masked length k, both in pre-training and fine-tuning stages, including PPL of pretrained model on English sentences (a), and French sentences (b) from WMT newstest2013 on English-French translation; the BLEU score of unsupervised English-French translation on WMT newstest2013 (c); the ROUGE score (F1 score in RG-2) on the validation set of text summarization (d); and the PPL on the validation set of conversational response generation (e).\" class=\"wp-image-593701 size-large\" height=\"202\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Fig-5-R-1024x202.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593701\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 5: MASS performance under various masked length k, both in pre-training and fine-tuning stages, including PPL of pretrained model on English sentences (a), and French sentences (b) from WMT newstest2013 on English-French translation; the BLEU score of unsupervised English-French translation on WMT newstest2013 (c); the ROUGE score (F1 score in RG-2) on the validation set of text summarization (d); and the PPL on the validation set of conversational response generation (e).</p></div></p>\n<p>When k equals half the sentence length, the downstream tasks can reach their best performance. Masking half the sentence can provide good balance in the pre-training of the encoder and decoder. Bias to the encoder (k=1, BERT), on the other hand, or bias to the decoder (k=m, LM/GPT) does not deliver good performance. This shows the advantages of MASS in sequence to sequence based language generation tasks.</p>\n<h3>Experimenting on sequence-to-sequence-based language generation tasks</h3>\n<p><strong>Pre-training</strong></p>\n<p>Notably, MASS only requires unsupervised monolingual data for pre-training (for example, WMT News Crawl Data or Wikipedia Data). MASS supports both cross-lingual tasks (for example, neural machine translation) and monolingual tasks (abstractive summarization and conversational response generation). When pre-training for cross-lingual tasks such as English-French translation, we pre-trained both English-English and French-French in one model, with an additional language embedding to differentiate between the languages. We fine-tuned MASS on unsupervised machine translation, low-resource machine translation, abstractive summarization and conversational response generation to verify its effectiveness.</p>\n<h3>Unsupervised machine translation</h3>\n<p>We compared MASS with previous methods, including the previous state-of-the-art method, Facebook XLM, on unsupervised machine translation tasks. XLM uses a masked language model in BERT and a standard language model to pre-train the encoder and decoder separately. As shown in Table 2, MASS outperforms XLM in six translation directions on WMT14 English-French, WMT16 English-German and English-Romanian, and achieves new state-of-the-art results.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_594874\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/Mass-Table-2-updated.png\"><img alt=\"Table 2: BLEU score comparisons between MASS and previous work on unsupervised NMT. Results on en-fr and fr-en pairs reported on newstest2014; others are on newstest2016. Because XLM uses different combinations of MLM and CLM in the encoder and decoder, we report the highest BLEU score for XLM on each language pair.\" class=\"wp-image-594874 size-large\" height=\"211\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/Mass-Table-2-updated-1024x211.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-594874\"><span class=\"sr-only\"> (opens in new tab)</span></a> Table 2: BLEU score comparisons between MASS and previous work on unsupervised NMT. Results on en-fr and fr-en pairs reported on newstest2014; others are on newstest2016. Because XLM uses different combinations of MLM and CLM in the encoder and decoder, we report the highest BLEU score for XLM on each language pair.</p></div></p>\n<h3>Low-resource machine translation</h3>\n<p>Low-resource machine translation refers to machine translation with limited bilingual training data. We simulated a low-resource scenario on WMT14 English-French, WMT16 English-German and English-Romanian translation (10K, 100K, and 1M bilingual data respectively).</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593743\" style=\"width: 1034px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-figure-6.png\"><img alt=\"Figure 6: The BLEU score comparisons between MASS and the baseline on low-resource NMT with different scales of paired data.\" class=\"wp-image-593743 size-large\" height=\"190\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-figure-6-1024x190.png\" width=\"1024\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593743\"><span class=\"sr-only\"> (opens in new tab)</span></a> Figure 6: The BLEU score comparisons between MASS and the baseline on low-resource NMT with different scales of paired data.</p></div></p>\n<p>Figure 6 shows MASS outperforms the low-resource baseline on different data scales and the improvement becomes larger with fewer bilingual data.</p>\n<h3>Abstractive Summarization</h3>\n<p>We compared MASS with BERT+LM (with the encoder pretrained with BERT and decoder pre-trained with LM) and DAE (Denoising Auto-Encoder) on the Gigaword Corpus. As can be seen in Table 3, MASS outperformed both BERT+LM and DAE.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_594877\" style=\"width: 621px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-table-3-udpated.png\"><img alt=\"Table 3: Comparisons between MASS and two pre-training methods for ROGUE score on text summarization task on the whole 3.8M training data.\" class=\"wp-image-594877\" height=\"185\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-table-3-udpated-1024x311.png\" width=\"611\" /><p class=\"wp-caption-text\" id=\"caption-attachment-594877\"><span class=\"sr-only\"> (opens in new tab)</span></a> Table 3: Comparisons between MASS and two pre-training methods for ROGUE score on text summarization task on the whole 3.8M training data.</p></div></p>\n<h3>Conversational response generation</h3>\n<p>We compared MASS with BERT+LM on Cornell Movie Dialog Corpus. Table 4 shows that MASS achieved lower PPL than BERT+LM and the baseline without any pre-training.</p>\n<p><div class=\"wp-caption aligncenter\" id=\"attachment_593713\" style=\"width: 621px;\"><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Table-4-R.png\"><img alt=\"Table 4: Comparisons between MASS and other baseline methods for PPL on Cornell Movie Dialog corpus.\" class=\"wp-image-593713 size-full\" height=\"163\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/MASS-Table-4-R.png\" width=\"611\" /><p class=\"wp-caption-text\" id=\"caption-attachment-593713\"><span class=\"sr-only\"> (opens in new tab)</span></a> Table 4: Comparisons between MASS and other baseline methods for PPL on Cornell Movie Dialog corpus.</p></div></p>\n<p>MASS consistently achieves significant gains on different sequence to sequence based language generation tasks. We are looking forward to testing the performance of MASS on natural language understanding tasks. Future work includes applying MASS to other sequence to sequence based generation tasks in the image and video domains.</p>\n<p>For more details, we invite you to peruse our paper, “<a href=\"https://www.microsoft.com/en-us/research/publication/mass-masked-sequence-to-sequence-pre-training-for-language-generation/\">MASS: Masked Sequence to Sequence Pre-training for Language Generation<span class=\"sr-only\"> (opens in new tab)</span></a>”. Our source code and the pre-trained models are also available on <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/MASS\">GitHub<span class=\"sr-only\"> (opens in new tab)</span></a>. We welcome your feedback!<span class=\"sr-only\" id=\"label-external-link\">Opens in a new tab</span></p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/\">Introducing MASS – A pre-training method that outperforms BERT and GPT in sequence to sequence language generation tasks</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
    "source_feed": "https://www.microsoft.com/en-us/research/blog/category/artificial-intelligence/feed/",
    "scraped_at_iso": "2025-05-03T23:50:16Z",
    "selected_image_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/Pre-Training-Method-MASS_Social_06_2019_1200x627.png",
    "filter_verdict": {
        "importance_level": "Interesting",
        "topic": "AI Models",
        "reasoning_summary": "The article reports on a new pre-training method (MASS) from Microsoft Research Asia that outperforms BERT and GPT in sequence-to-sequence language generation tasks, presenting verifiable performance claims and novel capabilities.",
        "primary_topic_keyword": "MASS pre-training method"
    },
    "filter_error": null,
    "filtered_at_iso": "2025-05-03T23:51:33Z",
    "topic": "AI Models",
    "is_breaking": false,
    "primary_keyword": "MASS pre-training method",
    "seo_agent_results": {
        "generated_title_tag": "MASS Pre-training Method Outperforms BERT and GPT in NLP",
        "generated_meta_description": "Microsoft's MASS pre-training method surpasses BERT and GPT in sequence-to-sequence language generation tasks, offering superior performance in NLP.",
        "generated_json_ld": "<script type=\"application/ld+json\">  \n{  \n  \"@context\": \"https://schema.org\",  \n  \"@type\": \"NewsArticle\",  \n  \"headline\": \"MASS Pre-training Method Outperforms BERT and GPT in NLP\",  \n  \"description\": \"Microsoft's MASS pre-training method surpasses BERT and GPT in sequence-to-sequence language generation tasks, offering superior performance in NLP.\",  \n  \"keywords\": [\"MASS pre-training method\"],  \n  \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/\" },  \n  \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://www.microsoft.com/en-us/research/wp-content/uploads/2019/06/Pre-Training-Method-MASS_Social_06_2019_1200x627.png\" },  \n  \"datePublished\": \"2019-06-24T01:00:22Z\",  \n  \"author\": { \"@type\": \"Person\", \"name\": \"AI News Team\" },  \n  \"publisher\": {  \n    \"@type\": \"Organization\",  \n    \"name\": \"Dacoola\",  \n    \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://dacoolaa.netlify.app\" }  \n  }  \n}  \n</script>",
        "generated_article_body_md": "## Introducing MASS – A Pre-training Method That Outperforms BERT and GPT in Sequence-to-Sequence Language Generation Tasks  \n\nMicrosoft Research Asia has introduced **MASS (Masked Sequence to Sequence Pre-training)**, a new NLP pre-training method that outperforms BERT and GPT in sequence-to-sequence language generation tasks. Unlike BERT and GPT, which focus separately on encoder or decoder training, MASS jointly trains the encoder-attention-decoder framework by masking and predicting sentence fragments. This approach enhances both language understanding and generation, achieving state-of-the-art results in tasks like machine translation, summarization, and conversational response generation.  \n\n### Why It Matters  \n\nMASS bridges a critical gap in NLP by optimizing sequence-to-sequence tasks, where traditional models like BERT and GPT fall short. Its ability to balance encoder and decoder training—especially when masking half the sentence—leads to superior performance in real-world applications like low-resource translation and abstractive summarization. By unifying pre-training for both understanding and generation, MASS could accelerate advancements in multilingual AI, automated content creation, and human-like conversational systems."
    },
    "seo_agent_error": null,
    "generated_tags": [
        "MASS pre-training",
        "NLP models",
        "sequence-to-sequence tasks",
        "BERT vs GPT",
        "machine translation",
        "abstractive summarization",
        "Microsoft Research Asia",
        "language generation",
        "encoder-decoder framework",
        "AI advancements"
    ],
    "tags_agent_error": null,
    "trend_score": 10.0,
    "audio_url": null
}