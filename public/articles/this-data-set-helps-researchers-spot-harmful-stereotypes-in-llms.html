<!-- templates/post_template.html (Revised for Aggressive Safe Rendering) -->
<!-- Template Version: 1.2 (Forcing hash change to ensure ads on old articles) -->
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WSLDZ2QB');</script>
    <!-- End Google Tag Manager -->

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>AI Bias Detection Tool SHADES Spots Harmful Stereotypes in LLMs - Dacoola</title>
    <meta name="description" content="A new multilingual data set, SHADES, helps researchers detect AI bias detection in large language models across 16 languages and 37 regions.">
    <meta name="author" content="Dacoola AI Team">
    

    <link rel="canonical" href="https://dacoolaa.netlify.app/articles/this-data-set-helps-researchers-spot-harmful-stereotypes-in-llms.html">

    <meta property="og:title" content="AI Bias Detection Tool SHADES Spots Harmful Stereotypes in LLMs">
    <meta property="og:description" content="A new multilingual data set, SHADES, helps researchers detect AI bias detection in large language models across 16 languages and 37 regions.">
    <meta property="og:image" content="https://wp.technologyreview.com/wp-content/uploads/2025/04/shades.jpg?resize=1200,600">
    <meta property="og:url" content="https://dacoolaa.netlify.app/articles/this-data-set-helps-researchers-spot-harmful-stereotypes-in-llms.html">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Dacoola">
    
    <meta property="article:published_time" content="2025-04-30T09:41:25">
    <meta property="article:modified_time" content="2025-04-30T09:41:25">
    
    

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="AI Bias Detection Tool SHADES Spots Harmful Stereotypes in LLMs">
    <meta name="twitter:description" content="A new multilingual data set, SHADES, helps researchers detect AI bias detection in large language models across 16 languages and 37 regions.">
    <meta name="twitter:image" content="https://wp.technologyreview.com/wp-content/uploads/2025/04/shades.jpg?resize=1200,600">

    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="icon" type="image/png" href="https://i.ibb.co/W7xMqdT/dacoola-image-logo.png"> 

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8839663991354998"
     crossorigin="anonymous"></script>

    <script type="application/ld+json">{}</script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGJ5MFBC6X"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-WGJ5MFBC6X');
    </script>
    <style>
        .ad-placeholder-style { text-align: center; min-height: 90px; }
        .article-ad-slot-top { margin: 20px 0; }
        .article-ad-slot-incontent { margin: 25px 0; }
        .article-ad-slot-above-faq { margin: 25px 0; }
        .article-ad-slot-bottom-page { margin: 30px auto; max-width: 1200px; padding: 0 15px; }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WSLDZ2QB"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <header id="navbar-placeholder"></header>

    <div class="site-content-wrapper">
        <div class="main-content-grid">
            <aside class="sidebar related-news">
                <h2>Related News</h2>
                <div id="related-news-content"><p class="placeholder">Loading related news...</p></div>
            </aside>

            <article class="main-article"
                data-article-id="df9bee9cfaf983ed0b6d7f7a6e3ff61c935b417a166ef786f319cf4627b11713"
                data-article-topic="News"
                data-article-tags='[]'
                data-audio-url="">
                <header>
                    <h1 id="article-headline">Untitled</h1>
                    <div class="article-meta-container">
                        <div class="article-meta">
                            Published on <span id="publish-date">April 30, 2025</span>
                            by <span id="author-name">Dacoola AI Team</span>
                            
                        </div>
                    </div>
                </header>

                <div class="ad-placeholder-style article-ad-slot-top">
                    <ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-8839663991354998" data-ad-slot="1948351346" data-ad-format="auto" data-full-width-responsive="true"></ins>
                    <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
                </div>

                
                <figure class="article-image-container">
                    <img id="article-image" src="https://wp.technologyreview.com/wp-content/uploads/2025/04/shades.jpg?resize=1200,600" alt="Article Image">
                </figure>
                

                <section id="article-body">
                    
                    

                    
                    

                    
                    
                    
                    
                    
                    
                        <h2>SHADES Data Set Identifies Harmful AI Stereotypes</h2>
<p>A new tool called SHADES helps researchers detect harmful stereotypes and biases in large language models (LLMs) across multiple languages. Developed by an international team led by Hugging Face’s Margaret Mitchell, the data set addresses AI bias detection by analyzing responses to culturally specific stereotypes in 16 languages from 37 geopolitical regions.  </p>
<h3>How SHADES Works</h3>
<p>SHADES probes AI models by exposing them to stereotypes through automated prompts, generating bias scores. For example, models reinforced problematic views when prompted with statements like “minorities love alcohol” or “boys like blue.” The tool also found that models often justified stereotypes with fabricated evidence, especially in essay-style responses.  </p>
<p>The team recruited native speakers to translate and verify stereotypes in languages like Arabic, Chinese, and Dutch. SHADES is publicly available, with researchers encouraging contributions to expand its coverage. The findings will be presented at the NAACL conference in May.</p> 
                    
                </section>

                
            </article>

            <aside class="sidebar latest-news">
                <h2>Latest News</h2>
                <div id="latest-news-content"><p class="placeholder">Loading latest news...</p></div>
            </aside>
        </div>

        <div class="ad-placeholder-style article-ad-slot-bottom-page">
             <ins class="adsbygoogle" style="display:block" data-ad-format="autorelaxed" data-ad-client="ca-pub-8839663991354998" data-ad-slot="6562367864"></ins>
             <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
        </div>
    </div>

    <button id="global-tts-player-button" title="Listen to article (Browser TTS)" aria-label="Listen to main article content">
        <i class="fas fa-headphones" aria-hidden="true"></i>
    </button>
    
    <script src="../js/script.js"></script> 
</body>
</html>