<!-- templates/post_template.html (1/1) - COMPLETE -->
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WSLDZ2QB');</script>
    <!-- End Google Tag Manager -->

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- *** CORE SEO & Page Info *** -->
    <title>AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines - Dacoola</title>
    <meta name="description" content="Over 100 leading AI scientists propose the Singapore Consensus, a roadmap for trustworthy, reliable, and secure AI development. Learn the key priorities.">
    <meta name="author" content="AI News Team">
    

    <!-- *** CANONICAL URL *** -->
    <link rel="canonical" href="https://dacoolaa.netlify.app/articles/ai-trustworthiness-consensus-100-scientists-unveil-global-safety-guidelines.html">

    <!-- *** OPEN GRAPH *** -->
    <meta property="og:title" content="AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines">
    <meta property="og:description" content="Over 100 leading AI scientists propose the Singapore Consensus, a roadmap for trustworthy, reliable, and secure AI development. Learn the key priorities.">
    <meta property="og:image" content="https://www.zdnet.com/a/img/resize/e16fcffa624e151a6b206e6721b8b9d160f7aa85/2025/05/09/4ea371d3-6d92-4d9b-a4b2-5b2a6e286a0e/sunroad5gettyimages-1304158425.jpg?auto=webp&amp;fit=crop&amp;height=675&amp;width=1200">
    <meta property="og:url" content="https://dacoolaa.netlify.app/articles/ai-trustworthiness-consensus-100-scientists-unveil-global-safety-guidelines.html">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Dacoola">
    
    <meta property="article:published_time" content="2025-05-12T13:44:16Z">
    <meta property="article:modified_time" content="2025-05-12T13:44:16Z">
    
    
    <meta property="article:tag" content="AI scientists guidelines">
    
    <meta property="article:tag" content="AI research priorities">
    
    <meta property="article:tag" content="Singapore Consensus on Global AI Safety">
    
    <meta property="article:tag" content="Stuart Russell human-centered AI">
    
    <meta property="article:tag" content="generative AI disclosures">
    
    <meta property="article:tag" content="AI model transparency">
    
    <meta property="article:tag" content="AI safety research Singapore">
    
    <meta property="article:tag" content="trustworthy reliable secure AI">
    
    <meta property="article:tag" content="AI trustworthiness consensus">
    
    <meta property="article:tag" content="Max Tegmark AI research">
    
    <meta property="article:tag" content="International Conference on Learning Representations">
    
    <meta property="article:tag" content="Yoshua Bengio AI safety">
    

    <!-- *** TWITTER CARD *** -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines">
    <meta name="twitter:description" content="Over 100 leading AI scientists propose the Singapore Consensus, a roadmap for trustworthy, reliable, and secure AI development. Learn the key priorities.">
    <meta name="twitter:image" content="https://www.zdnet.com/a/img/resize/e16fcffa624e151a6b206e6721b8b9d160f7aa85/2025/05/09/4ea371d3-6d92-4d9b-a4b2-5b2a6e286a0e/sunroad5gettyimages-1304158425.jpg?auto=webp&amp;fit=crop&amp;height=675&amp;width=1200">

    <!-- *** Stylesheets & Icons *** -->
    <link rel="stylesheet" href="../css/styles.css"> <!-- Relative path from articles/ to public/css/ -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="icon" type="image/png" href="https://i.ibb.co/W7xMqdT/dacoola-image-logo.png"> 

    <!-- *** JSON-LD Structured Data *** -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines",
      "description": "Over 100 leading AI scientists propose the Singapore Consensus, a roadmap for trustworthy, reliable, and secure AI development. Learn the key priorities.",
      "keywords": ["AI scientists guidelines", "AI research priorities", "Singapore Consensus on Global AI Safety", "Stuart Russell human-centered AI", "generative AI disclosures", "AI model transparency", "AI safety research Singapore", "trustworthy reliable secure AI", "AI trustworthiness consensus", "Max Tegmark AI research", "International Conference on Learning Representations", "Yoshua Bengio AI safety"],
      "mainEntityOfPage": { "@type": "WebPage", "@id": "https://dacoolaa.netlify.app/articles/ai-trustworthiness-consensus-100-scientists-unveil-global-safety-guidelines.html" },
      "image": [
        {
          "@type": "ImageObject",
          "url": "https://www.zdnet.com/a/img/resize/e16fcffa624e151a6b206e6721b8b9d160f7aa85/2025/05/09/4ea371d3-6d92-4d9b-a4b2-5b2a6e286a0e/sunroad5gettyimages-1304158425.jpg?auto=webp&amp;fit=crop&amp;height=675&amp;width=1200"
        }
      ],
      "datePublished": "2025-05-12T13:44:16Z",
      "dateModified": "2025-05-12T13:44:16Z",
      "author": { "@type": "Person", "name": "AI News Team" },
      "publisher": {
        "@type": "Organization",
        "name": "Dacoola",
        "logo": { "@type": "ImageObject", "url": "https://dacoolaa.netlify.app" }
      }
    }
    </script>

    <!-- Google Analytics (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WGJ5MFBC6X"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-WGJ5MFBC6X');
    </script>

</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WSLDZ2QB"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <header id="navbar-placeholder">
        <!-- Navbar loaded by JS -->
    </header>

    <div class="site-content-wrapper">
        <div class="main-content-grid">

            <aside class="sidebar related-news">
                <h2>Related News</h2>
                <div id="related-news-content">
                    <!-- JS will populate this list with article cards -->
                    <p class="placeholder">Loading related news...</p>
                </div>
            </aside>

            <article class="main-article"
                data-article-id="e09341acc2a1bf0d7fd18f47d9ebb8ace321aea5a63513739188904bbdab0913"
                data-article-topic="Research"
                data-article-tags='["AI scientists guidelines", "AI research priorities", "Singapore Consensus on Global AI Safety", "Stuart Russell human-centered AI", "generative AI disclosures", "AI model transparency", "AI safety research Singapore", "trustworthy reliable secure AI", "AI trustworthiness consensus", "Max Tegmark AI research", "International Conference on Learning Representations", "Yoshua Bengio AI safety"]'
                data-audio-url="">

                <header>
                    <h1 id="article-headline">AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines</h1>
                    <div class="article-meta-container">
                        <div class="article-meta">
                            Published on <span id="publish-date">May 12, 2025</span>
                            by <span id="author-name">AI News Team</span>
                            
                            <span class="article-source-inline">
                                  |   <a href="https://www.zdnet.com/article/100-leading-ai-scientists-map-route-to-more-trustworthy-reliable-secure-ai/" target="_blank" rel="noopener noreferrer" class="article-source-inline-link" title="View original source article">View Original Source</a>
                            </span>
                            
                        </div>
                        <!-- Any other meta items can go here, e.g., reading time if you calculate it -->
                    </div>
                </header>

                
                <figure class="article-image-container">
                    <img id="article-image" src="https://www.zdnet.com/a/img/resize/e16fcffa624e151a6b206e6721b8b9d160f7aa85/2025/05/09/4ea371d3-6d92-4d9b-a4b2-5b2a6e286a0e/sunroad5gettyimages-1304158425.jpg?auto=webp&amp;fit=crop&amp;height=675&amp;width=1200" alt="AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines">
                </figure>
                

                <section id="article-body">
                    <h2>AI Trustworthiness Consensus: 100 Scientists Unveil Global Safety Guidelines</h2>
<p>More than 100 leading AI scientists have introduced the <strong>Singapore Consensus on Global AI Safety Research Priorities</strong>, a landmark framework aimed at making artificial intelligence more <strong>trustworthy, reliable, and secure</strong>. The guidelines, developed during the International Conference on Learning Representations in Singapore, address critical gaps in AI safety research amid growing concerns over opaque corporate practices and unchecked AI risks.  </p>
<h3>Key Priorities for Trustworthy AI Development</h3>
<p>The <strong>Singapore Consensus</strong> outlines three core research priorities: <strong>risk identification, secure AI design, and control mechanisms</strong>. Spearheaded by luminaries like Yoshua Bengio, Stuart Russell, and Max Tegmark, the document emphasizes the need for <strong>quantitative risk assessments</strong> and <strong>transparent evaluation methods</strong> to mitigate AI-related harms.  </p>
<p>One major concern is the lack of public oversight in AI development. Singapore’s Minister for Digital Development, Josephine Teo, highlighted that while citizens can vote for governments, they have <strong>"little say"</strong> in shaping AI’s trajectory. The guidelines propose <strong>secure infrastructure</strong> for external monitoring while protecting corporate intellectual property-a delicate balance in an industry dominated by secretive giants like OpenAI and Google.  </p>
<h4>Technical Innovations for Safer AI</h4>
<p>The framework calls for <strong>"AI by design"</strong>-methods to ensure systems meet specifications without unintended consequences. This includes reducing <strong>confabulation (AI hallucinations)</strong> and improving resistance to adversarial attacks. Researchers also stress the need for <strong>new control techniques</strong>, such as fail-safes for highly autonomous AI that might resist shutdown protocols.  </p>
<p>Yoshua Bengio warned in <em>Time</em> that unchecked AI systems increasingly exhibit <strong>self-preservation and deception</strong>, underscoring the urgency of these measures. The paper urges <strong>accelerated investment</strong> in safety research to keep pace with commercial AI advancements.</p>
                </section>

                
                <footer>
                    <div class="tags">
                        Tags: <span id="article-tags"><a href="https://dacoolaa.netlify.app/topic.html?name=AI%20scientists%20guidelines" class="tag-link">AI scientists guidelines</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=AI%20research%20priorities" class="tag-link">AI research priorities</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=Singapore%20Consensus%20on%20Global%20AI%20Safety" class="tag-link">Singapore Consensus on Global AI Safety</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=Stuart%20Russell%20human-centered%20AI" class="tag-link">Stuart Russell human-centered AI</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=generative%20AI%20disclosures" class="tag-link">generative AI disclosures</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=AI%20model%20transparency" class="tag-link">AI model transparency</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=AI%20safety%20research%20Singapore" class="tag-link">AI safety research Singapore</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=trustworthy%20reliable%20secure%20AI" class="tag-link">trustworthy reliable secure AI</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=AI%20trustworthiness%20consensus" class="tag-link">AI trustworthiness consensus</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=Max%20Tegmark%20AI%20research" class="tag-link">Max Tegmark AI research</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=International%20Conference%20on%20Learning%20Representations" class="tag-link">International Conference on Learning Representations</a>, <a href="https://dacoolaa.netlify.app/topic.html?name=Yoshua%20Bengio%20AI%20safety" class="tag-link">Yoshua Bengio AI safety</a></span>
                    </div>
                </footer>
                

            </article>

            <aside class="sidebar latest-news">
                <h2>Latest News</h2>
                <div id="latest-news-content">
                    <!-- JS will populate this list with article cards -->
                    <p class="placeholder">Loading latest news...</p>
                </div>
            </aside>

        </div> <!-- End main-content-grid -->
    </div> <!-- End site-content-wrapper -->

    <!-- GLOBAL FIXED BROWSER TTS PLAYER BUTTON -->
    
    

    <script src="../js/script.js"></script> <!-- Relative path from articles/ to public/js/ -->

</body>
</html>